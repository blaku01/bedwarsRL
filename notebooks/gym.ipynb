{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:56:13.776305Z",
     "start_time": "2024-06-01T18:56:10.745549Z"
    }
   },
   "source": [
    "from src.environments.gym_1_vs_1 import MinecraftGym\n",
    "from src.train.train_dqn import train_dqn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "66fa4289cc31a396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:56:24.432495Z",
     "start_time": "2024-06-01T18:56:13.778272Z"
    }
   },
   "source": [
    "mc_gym = MinecraftGym()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-01 20:56:13.780\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mInitializing MinecraftAgent with username bot1 and enemy bot2\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:14.168\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m39\u001B[0m - \u001B[1mMinecraftAgent initialized\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:19.181\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m22\u001B[0m - \u001B[34m\u001B[1mInitializing MinecraftAgent with username bot2 and enemy bot1\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:19.408\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m39\u001B[0m - \u001B[1mMinecraftAgent initialized\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c7125349cc894a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:56:34.513134Z",
     "start_time": "2024-06-01T18:56:24.434490Z"
    }
   },
   "source": [
    "train_dqn(mc_gym, num_episodes=1000, batch_size=64)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-06-01 20:56:24.439\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m44\u001B[0m - \u001B[1mStarting DQN training\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:24.441\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m46\u001B[0m - \u001B[1mUsing device: cpu\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:24.445\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m53\u001B[0m - \u001B[1mInitialized policy and target networks\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.472\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36m__init__\u001B[0m:\u001B[36m29\u001B[0m - \u001B[1mInitialized ReplayMemory with capacity 1000000\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.478\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.488\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [22.69306784 50.75660551  0.71900114 17.5        14.28099886  0.5\n",
      "  3.14159265  0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.489\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.490\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 1.0 at step 1\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.493\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.8286, 0.3765,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.494\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, False, False, True, False, 0.8285682797431946, 0.3764509856700897, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.496\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, False, False, True, False, 0.8285682797431946, 0.3764509856700897, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.501\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m92\u001B[0m - \u001B[1mInitial distance to enemy set: 22.693067843369622\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.502\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, False, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.506\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.507\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.8285682797431946, yaw: 0.3764509856700897\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.510\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.513\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.693067843369622\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.515\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.516\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, False, False, True, False, 0.8285682797431946, 0.3764509856700897, False], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.517\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.523\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [22.69306784 50.75660551  0.71900114 17.5        14.28099886  0.5\n",
      "  0.82728607  0.37699112]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.524\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [22.69306784 50.75660551  0.71900114 17.5        14.28099886  0.5\n",
      "  0.82728607  0.37699112]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.526\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.527\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.531\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  3.1416,  0.0000]]), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.8286, 0.3765,\n",
      "        0.0000]), tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  0.8273,  0.3770]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.535\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  3.1416,  0.0000]]), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.8286, 0.3765,\n",
      "        0.0000]), next_state=tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  0.8273,  0.3770]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.538\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.540\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9982017988005998 at step 2\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.543\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3086, 0.5482,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.545\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, True, False, False, False, 0.3086106777191162, 0.5482240915298462, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.547\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, True, False, False, False, 0.3086106777191162, 0.5482240915298462, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.552\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m92\u001B[0m - \u001B[1mInitial distance to enemy set: 17.58440360091863\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.553\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.565\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.566\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.3086106777191162, yaw: 0.5482240915298462\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.571\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.580\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 17.58440360091863\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.581\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.583\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, True, False, False, False, 0.3086106777191162, 0.5482240915298462, False], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.614\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 1.70097215e+01 -9.19372070e+01  1.50750000e+01  5.00000000e-01\n",
      "  7.50000000e-02  1.75000000e+01  3.14159265e+00  0.00000000e+00]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.616\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 1.70097215e+01 -9.19372070e+01  1.50750000e+01  5.00000000e-01\n",
      "  7.50000000e-02  1.75000000e+01  3.14159265e+00  0.00000000e+00]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.617\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.618\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.623\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  0.8273,  0.3770]]), tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3086, 0.5482,\n",
      "        0.0000]), tensor([[ 1.7010e+01, -9.1937e+01,  1.5075e+01,  5.0000e-01,  7.5000e-02,\n",
      "          1.7500e+01,  3.1416e+00,  0.0000e+00]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.628\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[22.6931, 50.7566,  0.7190, 17.5000, 14.2810,  0.5000,  0.8273,  0.3770]]), action=tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3086, 0.5482,\n",
      "        0.0000]), next_state=tensor([[ 1.7010e+01, -9.1937e+01,  1.5075e+01,  5.0000e-01,  7.5000e-02,\n",
      "          1.7500e+01,  3.1416e+00,  0.0000e+00]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.629\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.631\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9964071904095924 at step 3\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.634\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7113, 0.7677,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.637\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, True, False, True, True, 0.7113178968429565, 0.7676764130592346, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.639\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, True, False, True, True, 0.7113178968429565, 0.7676764130592346, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.650\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, True, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.667\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.670\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7113178968429565, yaw: 0.7676764130592346\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.677\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.688\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.691\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.06703522978243832\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.692\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, True, False, True, True, 0.7113178968429565, 0.7676764130592346, False], reward: -0.06703522978243832\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.693\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.720\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.71209433   0.76707221]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.722\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.71209433   0.76707221]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.723\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.725\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.728\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 1.7010e+01, -9.1937e+01,  1.5075e+01,  5.0000e-01,  7.5000e-02,\n",
      "          1.7500e+01,  3.1416e+00,  0.0000e+00]]), tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7113, 0.7677,\n",
      "        0.0000]), tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.7121,\n",
      "           0.7671]]), tensor([-0.0670]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.731\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 1.7010e+01, -9.1937e+01,  1.5075e+01,  5.0000e-01,  7.5000e-02,\n",
      "          1.7500e+01,  3.1416e+00,  0.0000e+00]]), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7113, 0.7677,\n",
      "        0.0000]), next_state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.7121,\n",
      "           0.7671]]), reward=tensor([-0.0670])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.732\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.734\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9946161676485418 at step 4\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.737\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.7719, 0.9054,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.740\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, True, True, 0.7719160914421082, 0.9053695201873779, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.741\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, True, True, 0.7719160914421082, 0.9053695201873779, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.750\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.773\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.775\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7719160914421082, yaw: 0.9053695201873779\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.782\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.789\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.791\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.4438311944626609\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.793\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, True, True, 0.7719160914421082, 0.9053695201873779, False], reward: 0.4438311944626609\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.796\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.821\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.77230819   0.90582588]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.822\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.77230819   0.90582588]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.824\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.825\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.829\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.7121,\n",
      "           0.7671]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.7719, 0.9054,\n",
      "        0.0000]), tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7723,\n",
      "           0.9058]]), tensor([0.4438]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.832\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.7121,\n",
      "           0.7671]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.7719, 0.9054,\n",
      "        0.0000]), next_state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7723,\n",
      "           0.9058]]), reward=tensor([0.4438])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.834\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.835\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9928287233533546 at step 5\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.838\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.4141, 0.2425,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.840\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, True, True, False, True, 0.4140724837779999, 0.24254918098449707, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.842\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, True, True, False, True, 0.4140724837779999, 0.24254918098449707, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.850\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.858\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.860\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.4140724837779999, yaw: 0.24254918098449707\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.870\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.873\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.884\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.895\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.897\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.899\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, True, True, False, True, 0.4140724837779999, 0.24254918098449707, True], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.901\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.929\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.41364303   0.24347343]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.931\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.41364303   0.24347343]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.933\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.935\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.940\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7723,\n",
      "           0.9058]]), tensor([0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.4141, 0.2425,\n",
      "        1.0000]), tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.4136,\n",
      "           0.2435]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.945\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7723,\n",
      "           0.9058]]), action=tensor([0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.4141, 0.2425,\n",
      "        1.0000]), next_state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.4136,\n",
      "           0.2435]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.948\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.949\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9910448503742513 at step 6\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.952\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.7068, 0.1666,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.954\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, True, False, False, 0.7067523002624512, 0.16655004024505615, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.956\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, True, False, False, 0.7067523002624512, 0.16655004024505615, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.966\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.983\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.986\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7067523002624512, yaw: 0.16655004024505615\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.992\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:25.998\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.001\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.002\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, True, False, False, 0.7067523002624512, 0.16655004024505615, False], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.004\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.035\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.70685835   0.16755161]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.038\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.70685835   0.16755161]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.039\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.042\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.047\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.4136,\n",
      "           0.2435]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.7068, 0.1666,\n",
      "        0.0000]), tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7069,\n",
      "           0.1676]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.049\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.4136,\n",
      "           0.2435]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.7068, 0.1666,\n",
      "        0.0000]), next_state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7069,\n",
      "           0.1676]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.050\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.052\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9892645415757375 at step 7\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.054\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.6681, 0.0169,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.055\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, True, True, False, False, 0.6681187748908997, 0.016863718628883362, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.057\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, True, True, False, False, 0.6681187748908997, 0.016863718628883362, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.066\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, True, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.085\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.087\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6681187748908997, yaw: 0.016863718628883362\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.095\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.100\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.102\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.104\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, True, True, False, False, 0.6681187748908997, 0.016863718628883362, False], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.106\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.129\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20227155e+01 1.29472460e+02 1.45000000e+01 1.75000000e+01\n",
      " 5.00000000e-01 5.00000000e-01 6.67588439e-01 1.57079633e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.131\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.20227155e+01 1.29472460e+02 1.45000000e+01 1.75000000e+01\n",
      " 5.00000000e-01 5.00000000e-01 6.67588439e-01 1.57079633e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.132\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.133\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.137\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7069,\n",
      "           0.1676]]), tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.6681, 0.0169,\n",
      "        0.0000]), tensor([[2.2023e+01, 1.2947e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         6.6759e-01, 1.5708e-02]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.141\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.7069,\n",
      "           0.1676]]), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.6681, 0.0169,\n",
      "        0.0000]), next_state=tensor([[2.2023e+01, 1.2947e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         6.6759e-01, 1.5708e-02]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.143\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.145\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9874877898365757 at step 8\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.147\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.6144, 0.8862,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.149\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, False, True, True, 0.6143892407417297, 0.8862356543540955, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.150\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, False, True, True, 0.6143892407417297, 0.8862356543540955, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.160\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.167\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.168\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6143892407417297, yaw: 0.8862356543540955\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.172\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.181\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.182\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.184\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, False, True, True, 0.6143892407417297, 0.8862356543540955, False], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.186\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.214\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.61522856   0.88749992]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.216\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.61522856   0.88749992]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.217\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.218\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.221\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.2023e+01, 1.2947e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         6.6759e-01, 1.5708e-02]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.6144, 0.8862,\n",
      "        0.0000]), tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.6152,\n",
      "           0.8875]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.223\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.2023e+01, 1.2947e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         6.6759e-01, 1.5708e-02]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.6144, 0.8862,\n",
      "        0.0000]), next_state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.6152,\n",
      "           0.8875]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.224\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.226\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9857145880497566 at step 9\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.227\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4486, 0.1274,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.229\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, False, False, False, True, 0.44863155484199524, 0.1274237334728241, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.230\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, False, False, False, True, 0.44863155484199524, 0.1274237334728241, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.242\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, False, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.253\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.254\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.44863155484199524, yaw: 0.1274237334728241\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.257\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.258\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.265\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.274\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.277\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.278\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, False, False, False, True, 0.44863155484199524, 0.1274237334728241, True], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.279\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.294\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20133282e+01 1.29486410e+02 1.45000000e+01 1.75000000e+01\n",
      " 5.00000000e-01 5.00000000e-01 4.47676953e-01 1.28281700e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.297\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.20133282e+01 1.29486410e+02 1.45000000e+01 1.75000000e+01\n",
      " 5.00000000e-01 5.00000000e-01 4.47676953e-01 1.28281700e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.298\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.299\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.303\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.6152,\n",
      "           0.8875]]), tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4486, 0.1274,\n",
      "        1.0000]), tensor([[2.2013e+01, 1.2949e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         4.4768e-01, 1.2828e-01]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.306\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.6152,\n",
      "           0.8875]]), action=tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4486, 0.1274,\n",
      "        1.0000]), next_state=tensor([[2.2013e+01, 1.2949e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         4.4768e-01, 1.2828e-01]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.308\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.311\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9839449291224707 at step 10\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.314\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0652, 0.3147,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.316\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, False, True, True, False, 0.06520917266607285, 0.31469446420669556, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.317\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, False, True, True, False, 0.06520917266607285, 0.31469446420669556, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.329\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, False, True, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.343\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', False), ('jump', True), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.344\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.06520917266607285, yaw: 0.31469446420669556\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.348\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.354\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.996461933955583\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.355\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.002625361158965589\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.356\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, False, True, True, False, 0.06520917266607285, 0.31469446420669556, False], reward: -0.002625361158965589\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.357\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.378\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.99646193 -50.48356441   0.50366142   0.53100345  14.49633858\n",
      "  17.46899655   0.06544985   0.31415927]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.379\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.99646193 -50.48356441   0.50366142   0.53100345  14.49633858\n",
      "  17.46899655   0.06544985   0.31415927]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.380\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.382\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.385\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.2013e+01, 1.2949e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         4.4768e-01, 1.2828e-01]]), tensor([1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0652, 0.3147,\n",
      "        0.0000]), tensor([[ 21.9965, -50.4836,   0.5037,   0.5310,  14.4963,  17.4690,   0.0654,\n",
      "           0.3142]]), tensor([-0.0026]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.389\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.2013e+01, 1.2949e+02, 1.4500e+01, 1.7500e+01, 5.0000e-01, 5.0000e-01,\n",
      "         4.4768e-01, 1.2828e-01]]), action=tensor([1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0652, 0.3147,\n",
      "        0.0000]), next_state=tensor([[ 21.9965, -50.4836,   0.5037,   0.5310,  14.4963,  17.4690,   0.0654,\n",
      "           0.3142]]), reward=tensor([-0.0026])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.390\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.391\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9821788059760798 at step 11\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.393\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9253, 0.7623,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.395\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, False, False, False, False, 0.9252667427062988, 0.7622950673103333, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.396\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, False, False, False, False, 0.9252667427062988, 0.7622950673103333, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.401\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, False, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.408\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.409\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9252667427062988, yaw: 0.7622950673103333\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.413\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.415\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.418\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.425\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.963642034315452\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.427\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0059073511229787105\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.428\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, False, False, False, False, 0.9252667427062988, 0.7622950673103333, True], reward: -0.0059073511229787105\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.430\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.455\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.65113454 130.35444773  14.5         17.44095383   0.5\n",
      "   0.55904617   0.92415184   0.76183622]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.456\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.65113454 130.35444773  14.5         17.44095383   0.5\n",
      "   0.55904617   0.92415184   0.76183622]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.458\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.459\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.461\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9965, -50.4836,   0.5037,   0.5310,  14.4963,  17.4690,   0.0654,\n",
      "           0.3142]]), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9253, 0.7623,\n",
      "        1.0000]), tensor([[ 21.6511, 130.3544,  14.5000,  17.4410,   0.5000,   0.5590,   0.9242,\n",
      "           0.7618]]), tensor([-0.0059]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.464\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9965, -50.4836,   0.5037,   0.5310,  14.4963,  17.4690,   0.0654,\n",
      "           0.3142]]), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9253, 0.7623,\n",
      "        1.0000]), next_state=tensor([[ 21.6511, 130.3544,  14.5000,  17.4410,   0.5000,   0.5590,   0.9242,\n",
      "           0.7618]]), reward=tensor([-0.0059])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.466\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.468\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.980416211546089 at step 12\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.471\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.9042, 0.0792,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.473\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, False, True, False, False, 0.9041939377784729, 0.07916529476642609, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.474\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, False, True, False, False, 0.9041939377784729, 0.07916529476642609, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.486\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, False, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.492\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', False), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.494\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9041939377784729, yaw: 0.07916529476642609\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.495\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.497\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.500\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.506\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.53956384021835\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.508\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.04568980937372338\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.509\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, False, True, False, False, 0.9041939377784729, 0.07916529476642609, True], reward: -0.04568980937372338\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.510\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.540\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.55034272 -48.66938513   0.467224     1.16727113  14.532776\n",
      "  16.83272887   0.90320789   0.07853982]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.541\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.55034272 -48.66938513   0.467224     1.16727113  14.532776\n",
      "  16.83272887   0.90320789   0.07853982]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.543\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C8E78A00>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.544\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: False, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.547\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.6511, 130.3544,  14.5000,  17.4410,   0.5000,   0.5590,   0.9242,\n",
      "           0.7618]]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.9042, 0.0792,\n",
      "        1.0000]), tensor([[ 21.5503, -48.6694,   0.4672,   1.1673,  14.5328,  16.8327,   0.9032,\n",
      "           0.0785]]), tensor([-0.0457]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.549\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.6511, 130.3544,  14.5000,  17.4410,   0.5000,   0.5590,   0.9242,\n",
      "           0.7618]]), action=tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.9042, 0.0792,\n",
      "        1.0000]), next_state=tensor([[ 21.5503, -48.6694,   0.4672,   1.1673,  14.5328,  16.8327,   0.9032,\n",
      "           0.0785]]), reward=tensor([-0.0457])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.550\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.552\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9786571387821184 at step 13\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.554\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.1912, 0.5912,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.555\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, False, True, True, True, 0.19121375679969788, 0.5912221074104309, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.556\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, False, True, True, True, 0.19121375679969788, 0.5912221074104309, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.562\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, False, True, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.579\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.581\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.19121375679969788, yaw: 0.5912221074104309\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.585\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.588\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.592\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.598\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.37060615941301\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.599\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.05930358749024407\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.600\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, False, True, True, True, 0.19121375679969788, 0.5912221074104309, True], reward: -0.05930358749024407\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.601\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.622\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.23847774 131.88815634  14.62448539  17.37042629   0.37551461\n",
      "   0.62957371   0.19111355   0.59166662]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.624\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.23847774 131.88815634  14.62448539  17.37042629   0.37551461\n",
      "   0.62957371   0.19111355   0.59166662]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.626\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.627\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.630\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.5503, -48.6694,   0.4672,   1.1673,  14.5328,  16.8327,   0.9032,\n",
      "           0.0785]]), tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.1912, 0.5912,\n",
      "        1.0000]), tensor([[ 21.2385, 131.8882,  14.6245,  17.3704,   0.3755,   0.6296,   0.1911,\n",
      "           0.5917]]), tensor([-0.0593]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.633\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.5503, -48.6694,   0.4672,   1.1673,  14.5328,  16.8327,   0.9032,\n",
      "           0.0785]]), action=tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.1912, 0.5912,\n",
      "        1.0000]), next_state=tensor([[ 21.2385, 131.8882,  14.6245,  17.3704,   0.3755,   0.6296,   0.1911,\n",
      "           0.5917]]), reward=tensor([-0.0593])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.635\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.637\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 0 finished after 13 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.639\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.649\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.670\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.13772868 131.99237004  14.57574958  17.43976092   0.42425042\n",
      "   0.56023908   0.19111355   0.59166662]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.671\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 1\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.672\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9769015806478745 at step 14\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.674\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.1629, 0.5198,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.676\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, False, False, False, 0.16288703680038452, 0.5198495388031006, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.677\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, False, False, False, 0.16288703680038452, 0.5198495388031006, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.684\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.697\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.699\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.16288703680038452, yaw: 0.5198495388031006\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.702\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.708\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.149001732236812\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.710\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.022160442717619945\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.712\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, False, False, False, 0.16288703680038452, 0.5198495388031006, False], reward: -0.022160442717619945\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.713\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.736\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.04000105 132.07555498  14.52389517  17.50140345   0.47610483\n",
      "   0.49859655   0.16231562   0.52098078]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.738\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.04000105 132.07555498  14.52389517  17.50140345   0.47610483\n",
      "   0.49859655   0.16231562   0.52098078]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.739\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.740\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.743\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.1377, 131.9924,  14.5757,  17.4398,   0.4243,   0.5602,   0.1911,\n",
      "           0.5917]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.1629, 0.5198,\n",
      "        0.0000]), tensor([[ 21.0400, 132.0756,  14.5239,  17.5014,   0.4761,   0.4986,   0.1623,\n",
      "           0.5210]]), tensor([-0.0222]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.746\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.1377, 131.9924,  14.5757,  17.4398,   0.4243,   0.5602,   0.1911,\n",
      "           0.5917]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.1629, 0.5198,\n",
      "        0.0000]), next_state=tensor([[ 21.0400, 132.0756,  14.5239,  17.5014,   0.4761,   0.4986,   0.1623,\n",
      "           0.5210]]), reward=tensor([-0.0222])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.748\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.749\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 1 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.762\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.795\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 20.82960183 132.54210085  14.5         17.5          0.5\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.796\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 2\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.797\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9751495301211222 at step 15\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.798\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2762, 0.3592,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.801\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, False, False, True, True, 0.27621397376060486, 0.3592338263988495, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.802\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, False, False, True, True, 0.27621397376060486, 0.3592338263988495, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.810\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, False, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.830\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', False), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.832\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.27621397376060486, yaw: 0.3592338263988495\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.837\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.840\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.845\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.852\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 20.832218282571425\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.854\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.03167834496653867\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.856\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, False, False, True, True, 0.27621397376060486, 0.3592338263988495, True], reward: -0.03167834496653867\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.857\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.877\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02077177 129.44479234  14.49055431  17.50525741   0.50944569\n",
      "   0.49474259   0.27750735   0.35866516]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.878\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02077177 129.44479234  14.49055431  17.50525741   0.50944569\n",
      "   0.49474259   0.27750735   0.35866516]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.879\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.881\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.884\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 20.8296, 132.5421,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2762, 0.3592,\n",
      "        1.0000]), tensor([[ 22.0208, 129.4448,  14.4906,  17.5053,   0.5094,   0.4947,   0.2775,\n",
      "           0.3587]]), tensor([-0.0317]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.887\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 20.8296, 132.5421,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), action=tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.2762, 0.3592,\n",
      "        1.0000]), next_state=tensor([[ 22.0208, 129.4448,  14.4906,  17.5053,   0.5094,   0.4947,   0.2775,\n",
      "           0.3587]]), reward=tensor([-0.0317])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.888\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.891\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 2 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.897\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.917\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.01707342 129.39193387  14.47251303  17.51529905   0.52748697\n",
      "   0.48470095   0.27750735   0.35866516]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.919\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 3\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.921\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9734009801936573 at step 16\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.923\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6129, 0.1724,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.924\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, True, False, False, False, 0.6129053235054016, 0.17236573994159698, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.926\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, True, False, False, False, 0.6129053235054016, 0.17236573994159698, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.937\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.950\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.953\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6129053235054016, yaw: 0.17236573994159698\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.957\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.958\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.961\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.972\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.004176174731256\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.974\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.11719578921598313\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.976\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, True, False, False, False, 0.6129053235054016, 0.17236573994159698, True], reward: 0.11719578921598313\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.977\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.998\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.00417617 129.20531867  14.40886697  17.55072399   0.59113303\n",
      "   0.44927601   0.61261057   0.1727876 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:26.999\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.00417617 129.20531867  14.40886697  17.55072399   0.59113303\n",
      "   0.44927601   0.61261057   0.1727876 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.000\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.001\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.005\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0171, 129.3919,  14.4725,  17.5153,   0.5275,   0.4847,   0.2775,\n",
      "           0.3587]]), tensor([0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6129, 0.1724,\n",
      "        1.0000]), tensor([[ 22.0042, 129.2053,  14.4089,  17.5507,   0.5911,   0.4493,   0.6126,\n",
      "           0.1728]]), tensor([0.1172]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.009\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0171, 129.3919,  14.4725,  17.5153,   0.5275,   0.4847,   0.2775,\n",
      "           0.3587]]), action=tensor([0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6129, 0.1724,\n",
      "        1.0000]), next_state=tensor([[ 22.0042, 129.2053,  14.4089,  17.5507,   0.5911,   0.4493,   0.6126,\n",
      "           0.1728]]), reward=tensor([0.1172])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.011\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.013\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 3 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.022\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.052\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.97075727 129.34925045  14.43046676  17.48988731   0.56953324\n",
      "   0.51011269   0.61261057   0.1727876 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.053\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 4\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.055\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9716559238712779 at step 17\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.057\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4209, 0.8587,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.058\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, False, False, True, 0.4208712875843048, 0.8586759567260742, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.060\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, False, False, True, 0.4208712875843048, 0.8586759567260742, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.068\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.073\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.074\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.4208712875843048, yaw: 0.8586759567260742\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.078\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.083\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.926629436277246\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.085\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.007754673845401072\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.087\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, False, False, True, 0.4208712875843048, 0.8586759567260742, False], reward: -0.007754673845401072\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.089\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.111\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.86352764 129.88861608  14.52101891  17.27572262   0.47898109\n",
      "   0.72427738   0.42149701   0.85870199]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.113\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.86352764 129.88861608  14.52101891  17.27572262   0.47898109\n",
      "   0.72427738   0.42149701   0.85870199]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.115\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.118\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.122\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9708, 129.3492,  14.4305,  17.4899,   0.5695,   0.5101,   0.6126,\n",
      "           0.1728]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4209, 0.8587,\n",
      "        0.0000]), tensor([[ 21.8635, 129.8886,  14.5210,  17.2757,   0.4790,   0.7243,   0.4215,\n",
      "           0.8587]]), tensor([-0.0078]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.126\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9708, 129.3492,  14.4305,  17.4899,   0.5695,   0.5101,   0.6126,\n",
      "           0.1728]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4209, 0.8587,\n",
      "        0.0000]), next_state=tensor([[ 21.8635, 129.8886,  14.5210,  17.2757,   0.4790,   0.7243,   0.4215,\n",
      "           0.8587]]), reward=tensor([-0.0078])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.127\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.128\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 4 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.142\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.163\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.78993168 130.04174233  14.51845523  17.18184749   0.48154477\n",
      "   0.81815251   0.42149701   0.85870199]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.164\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 5\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.166\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9699143541737559 at step 18\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.168\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.4304, 0.0712,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.169\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, True, True, 0.43037381768226624, 0.0711916983127594, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.170\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, True, True, 0.43037381768226624, 0.0711916983127594, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.178\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.186\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.188\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.43037381768226624, yaw: 0.0711916983127594\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.193\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.198\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.787100053444135\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.200\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.013952938283311057\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.202\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, True, True, 0.43037381768226624, 0.0711916983127594, False], reward: -0.013952938283311057\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.203\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.225\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.17871001e+01 1.29701687e+02 1.44173918e+01 1.72625754e+01\n",
      " 5.82608216e-01 7.37424581e-01 4.29350996e-01 7.06858347e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.227\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.17871001e+01 1.29701687e+02 1.44173918e+01 1.72625754e+01\n",
      " 5.82608216e-01 7.37424581e-01 4.29350996e-01 7.06858347e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.227\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.230\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.233\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7899, 130.0417,  14.5185,  17.1818,   0.4815,   0.8182,   0.4215,\n",
      "           0.8587]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.4304, 0.0712,\n",
      "        0.0000]), tensor([[2.1787e+01, 1.2970e+02, 1.4417e+01, 1.7263e+01, 5.8261e-01, 7.3742e-01,\n",
      "         4.2935e-01, 7.0686e-02]]), tensor([-0.0140]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.235\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7899, 130.0417,  14.5185,  17.1818,   0.4815,   0.8182,   0.4215,\n",
      "           0.8587]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.4304, 0.0712,\n",
      "        0.0000]), next_state=tensor([[2.1787e+01, 1.2970e+02, 1.4417e+01, 1.7263e+01, 5.8261e-01, 7.3742e-01,\n",
      "         4.2935e-01, 7.0686e-02]]), reward=tensor([-0.0140])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.237\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.238\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 5 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.267\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.294\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.17756641e+01 1.29525297e+02 1.43584430e+01 1.72965206e+01\n",
      " 6.41557009e-01 7.03479381e-01 4.29350996e-01 7.06858347e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.296\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 6\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.297\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9681762641348107 at step 19\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.300\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2853, 0.0860,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.301\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, False, False, 0.28532513976097107, 0.0859772190451622, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.302\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, False, False, 0.28532513976097107, 0.0859772190451622, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.312\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.319\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.321\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.28532513976097107, yaw: 0.0859772190451622\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.326\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.331\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.755213741500487\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.333\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0031886311943647885\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.334\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, False, False, 0.28532513976097107, 0.0859772190451622, False], reward: -0.0031886311943647885\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.335\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.358\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.17562484e+01 1.29371640e+02 1.43010314e+01 1.73186169e+01\n",
      " 6.98968561e-01 6.81383126e-01 2.85361333e-01 8.63937980e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.359\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.17562484e+01 1.29371640e+02 1.43010314e+01 1.73186169e+01\n",
      " 6.98968561e-01 6.81383126e-01 2.85361333e-01 8.63937980e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.360\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.362\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.365\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1776e+01, 1.2953e+02, 1.4358e+01, 1.7297e+01, 6.4156e-01, 7.0348e-01,\n",
      "         4.2935e-01, 7.0686e-02]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2853, 0.0860,\n",
      "        0.0000]), tensor([[2.1756e+01, 1.2937e+02, 1.4301e+01, 1.7319e+01, 6.9897e-01, 6.8138e-01,\n",
      "         2.8536e-01, 8.6394e-02]]), tensor([-0.0032]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.368\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1776e+01, 1.2953e+02, 1.4358e+01, 1.7297e+01, 6.4156e-01, 7.0348e-01,\n",
      "         4.2935e-01, 7.0686e-02]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2853, 0.0860,\n",
      "        0.0000]), next_state=tensor([[2.1756e+01, 1.2937e+02, 1.4301e+01, 1.7319e+01, 6.9897e-01, 6.8138e-01,\n",
      "         2.8536e-01, 8.6394e-02]]), reward=tensor([-0.0032])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.371\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.372\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 6 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.382\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.401\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.17170769e+01 1.29245529e+02 1.42391978e+01 1.73186169e+01\n",
      " 7.60802230e-01 6.81383126e-01 2.85361333e-01 8.63937980e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.402\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 7\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.403\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9664416468020796 at step 20\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.406\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.1195, 0.2244,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.407\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, True, False, True, False, 0.11945252120494843, 0.22435691952705383, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.408\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, True, False, True, False, 0.11945252120494843, 0.22435691952705383, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.417\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, True, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.425\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.426\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.11945252120494843, yaw: 0.22435691952705383\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.429\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.431\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.434\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.442\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.661856580356\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.444\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.009335716114448545\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.446\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, True, False, True, False, 0.11945252120494843, 0.22435691952705383, True], reward: -0.009335716114448545\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.448\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.474\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.16116703e+01 1.29043490e+02 1.41134094e+01 1.72850939e+01\n",
      " 8.86590622e-01 7.14906105e-01 1.20427718e-01 2.25147474e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.476\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.16116703e+01 1.29043490e+02 1.41134094e+01 1.72850939e+01\n",
      " 8.86590622e-01 7.14906105e-01 1.20427718e-01 2.25147474e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.477\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.478\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.482\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1717e+01, 1.2925e+02, 1.4239e+01, 1.7319e+01, 7.6080e-01, 6.8138e-01,\n",
      "         2.8536e-01, 8.6394e-02]]), tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.1195, 0.2244,\n",
      "        1.0000]), tensor([[2.1612e+01, 1.2904e+02, 1.4113e+01, 1.7285e+01, 8.8659e-01, 7.1491e-01,\n",
      "         1.2043e-01, 2.2515e-01]]), tensor([-0.0093]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.485\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1717e+01, 1.2925e+02, 1.4239e+01, 1.7319e+01, 7.6080e-01, 6.8138e-01,\n",
      "         2.8536e-01, 8.6394e-02]]), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.1195, 0.2244,\n",
      "        1.0000]), next_state=tensor([[2.1612e+01, 1.2904e+02, 1.4113e+01, 1.7285e+01, 8.8659e-01, 7.1491e-01,\n",
      "         1.2043e-01, 2.2515e-01]]), reward=tensor([-0.0093])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.486\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.486\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 7 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.494\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.512\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.15660543e+01 1.28955281e+02 1.40588725e+01 1.72705597e+01\n",
      " 9.41127463e-01 7.29440335e-01 1.20427718e-01 2.25147474e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.513\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 8\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.514\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9647104952370908 at step 21\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.516\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.9747, 0.3147,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.518\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, False, True, True, False, 0.974674642086029, 0.31471163034439087, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.521\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, False, True, True, False, 0.974674642086029, 0.31471163034439087, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.528\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, False, True, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.544\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', False), ('jump', True), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.545\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.974674642086029, yaw: 0.31471163034439087\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.549\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.550\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.553\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.562\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.524588272813663\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.563\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.013726830754233888\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.564\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, False, True, True, False, 0.974674642086029, 0.31471163034439087, True], reward: -0.013726830754233888\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.565\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.583\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.52458827 128.8746861   14.00924401  17.25733352   0.99075599\n",
      "   0.74266648   0.97389372   0.31415927]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.52458827 128.8746861   14.00924401  17.25733352   0.99075599\n",
      "   0.74266648   0.97389372   0.31415927]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.586\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.587\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.590\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1566e+01, 1.2896e+02, 1.4059e+01, 1.7271e+01, 9.4113e-01, 7.2944e-01,\n",
      "         1.2043e-01, 2.2515e-01]]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.9747, 0.3147,\n",
      "        1.0000]), tensor([[ 21.5246, 128.8747,  14.0092,  17.2573,   0.9908,   0.7427,   0.9739,\n",
      "           0.3142]]), tensor([-0.0137]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.593\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1566e+01, 1.2896e+02, 1.4059e+01, 1.7271e+01, 9.4113e-01, 7.2944e-01,\n",
      "         1.2043e-01, 2.2515e-01]]), action=tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.9747, 0.3147,\n",
      "        1.0000]), next_state=tensor([[ 21.5246, 128.8747,  14.0092,  17.2573,   0.9908,   0.7427,   0.9739,\n",
      "           0.3142]]), reward=tensor([-0.0137])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.595\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.597\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 8 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.605\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.627\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.48689138 128.80107444  13.96408205  17.24529772   1.03591795\n",
      "   0.75470228   0.97389372   0.31415927]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.628\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 9\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.629\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.962982802515236 at step 22\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.631\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.8677, 0.7943,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.632\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, False, False, False, 0.8677489161491394, 0.7943009734153748, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.633\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, False, False, False, 0.8677489161491394, 0.7943009734153748, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.642\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.650\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.651\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.8677489161491394, yaw: 0.7943009734153748\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.655\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.656\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.659\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.663\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.452618137093605\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.664\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.007197013572005773\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.665\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, False, False, False, 0.8677489161491394, 0.7943009734153748, True], reward: -0.007197013572005773\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.667\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.683\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.45261814 128.73386311  13.92298467  17.23434515   1.07701533\n",
      "   0.76565485   0.86655597   0.79325215]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.685\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.45261814 128.73386311  13.92298467  17.23434515   1.07701533\n",
      "   0.76565485   0.86655597   0.79325215]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.686\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.687\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.690\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.4869, 128.8011,  13.9641,  17.2453,   1.0359,   0.7547,   0.9739,\n",
      "           0.3142]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.8677, 0.7943,\n",
      "        1.0000]), tensor([[ 21.4526, 128.7339,  13.9230,  17.2343,   1.0770,   0.7657,   0.8666,\n",
      "           0.7933]]), tensor([-0.0072]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.693\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.4869, 128.8011,  13.9641,  17.2453,   1.0359,   0.7547,   0.9739,\n",
      "           0.3142]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.8677, 0.7943,\n",
      "        1.0000]), next_state=tensor([[ 21.4526, 128.7339,  13.9230,  17.2343,   1.0770,   0.7657,   0.8666,\n",
      "           0.7933]]), reward=tensor([-0.0072])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.695\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.697\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 9 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.705\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.726\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.49431507 128.48962392  13.87747918  17.32404919   1.12252082\n",
      "   0.67595081   0.86655597   0.79325215]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.728\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 10\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.730\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.961258561725742 at step 23\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.732\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5210, 0.6707,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.734\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, True, False, False, False, 0.5209800601005554, 0.6706672310829163, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.735\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, True, False, False, False, 0.5209800601005554, 0.6706672310829163, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.742\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.749\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.750\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.5209800601005554, yaw: 0.6706672310829163\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.754\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.755\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.759\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.763\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.590481149488166\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.765\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.013786301239456122\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.766\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, True, False, False, False, 0.5209800601005554, 0.6706672310829163, True], reward: 0.013786301239456122\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.767\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.788\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.59048115 128.17566174  13.8445263   17.47269849   1.1554737\n",
      "   0.52730151   0.52098078   0.67020643]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.790\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.59048115 128.17566174  13.8445263   17.47269849   1.1554737\n",
      "   0.52730151   0.52098078   0.67020643]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.791\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.793\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.796\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.4943, 128.4896,  13.8775,  17.3240,   1.1225,   0.6760,   0.8666,\n",
      "           0.7933]]), tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5210, 0.6707,\n",
      "        1.0000]), tensor([[ 21.5905, 128.1757,  13.8445,  17.4727,   1.1555,   0.5273,   0.5210,\n",
      "           0.6702]]), tensor([0.0138]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.798\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.4943, 128.4896,  13.8775,  17.3240,   1.1225,   0.6760,   0.8666,\n",
      "           0.7933]]), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.5210, 0.6707,\n",
      "        1.0000]), next_state=tensor([[ 21.5905, 128.1757,  13.8445,  17.4727,   1.1555,   0.5273,   0.5210,\n",
      "           0.6702]]), reward=tensor([0.0138])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.799\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.800\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 10 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.801\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 10\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.809\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.830\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.64325956 128.00541869  13.82653403  17.55386101   1.17346597\n",
      "   0.44613899   0.52098078   0.67020643]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.832\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 11\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.833\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9595377659716432 at step 24\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.835\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3760, 0.3003,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.837\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, False, True, False, 0.3760277032852173, 0.3003005385398865, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.838\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, False, True, False, 0.3760277032852173, 0.3003005385398865, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.846\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.855\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.857\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.3760277032852173, yaw: 0.3003005385398865\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.860\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.875\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.877\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.04322343960570727\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.878\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, False, True, False, 0.3760277032852173, 0.3003005385398865, False], reward: 0.04322343960570727\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.879\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.905\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.906\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.908\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.908\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.911\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.6433, 128.0054,  13.8265,  17.5539,   1.1735,   0.4461,   0.5210,\n",
      "           0.6702]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3760, 0.3003,\n",
      "        0.0000]), tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), tensor([0.0432]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.914\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.6433, 128.0054,  13.8265,  17.5539,   1.1735,   0.4461,   0.5210,\n",
      "           0.6702]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3760, 0.3003,\n",
      "        0.0000]), next_state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), reward=tensor([0.0432])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.915\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.917\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 11 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.927\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.957\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.958\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 12\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.960\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9578204083697542 at step 25\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.966\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m71\u001B[0m - \u001B[1mpolicy net output: tensor([1.3581e-03, 8.7867e-04, 7.2253e-01, 9.9843e-01, 5.5203e-01, 1.1513e-02,\n",
      "        8.6650e-01, 9.8481e-01, 6.8298e-01, 9.9897e-01])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.971\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mSelected action from policy network: tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9848, 0.6830,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.973\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, True, False, True, 0.9848141074180603, 0.682977020740509, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.975\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, True, False, True, 0.9848141074180603, 0.682977020740509, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.987\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.996\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:27.998\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9848141074180603, yaw: 0.682977020740509\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.002\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.004\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.009\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.016\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02271554554524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.018\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.020\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, True, False, True, 0.9848141074180603, 0.682977020740509, True], reward: 0.0\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.021\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.049\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.9843657    0.6832964 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.051\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.9843657    0.6832964 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.053\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.054\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.058\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9848, 0.6830,\n",
      "        1.0000]), tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.9844,\n",
      "           0.6833]]), tensor([0.]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.060\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9848, 0.6830,\n",
      "        1.0000]), next_state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.9844,\n",
      "           0.6833]]), reward=tensor([0.])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.062\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.064\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 12 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.073\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.108\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.9843657    0.6832964 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.110\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 13\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.111\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9561064820506426 at step 26\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.114\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2417, 0.6638,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.115\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, False, True, 0.2417440265417099, 0.6637704968452454, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.117\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, False, True, 0.2417440265417099, 0.6637704968452454, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.126\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.133\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.134\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.2417440265417099, yaw: 0.6637704968452454\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.141\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.142\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.146\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.154\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.01448652536172\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.156\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0008229020183520675\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.158\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, False, True, 0.2417440265417099, 0.6637704968452454, True], reward: -0.0008229020183520675\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.159\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.182\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.01448653 129.47557378  14.49569231  17.49288715   0.50430769\n",
      "   0.50711285   0.24085544   0.66497045]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.184\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.01448653 129.47557378  14.49569231  17.49288715   0.50430769\n",
      "   0.50711285   0.24085544   0.66497045]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.185\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.187\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.189\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.9844,\n",
      "           0.6833]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2417, 0.6638,\n",
      "        1.0000]), tensor([[ 22.0145, 129.4756,  14.4957,  17.4929,   0.5043,   0.5071,   0.2409,\n",
      "           0.6650]]), tensor([-0.0008]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.193\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.9844,\n",
      "           0.6833]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2417, 0.6638,\n",
      "        1.0000]), next_state=tensor([[ 22.0145, 129.4756,  14.4957,  17.4929,   0.5043,   0.5071,   0.2409,\n",
      "           0.6650]]), reward=tensor([-0.0008])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.194\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.196\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 13 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.204\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.226\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.99876928 129.48152785  14.48746461  17.47930162   0.51253539\n",
      "   0.52069838   0.24085544   0.66497045]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.228\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 14\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.229\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9543959801586006 at step 27\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.231\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.7454, 0.9595,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.233\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, True, False, False, True, 0.7453998923301697, 0.9595336318016052, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.235\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, True, False, False, True, 0.7453998923301697, 0.9595336318016052, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.241\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, True, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.252\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.253\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7453998923301697, yaw: 0.9595336318016052\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.256\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.257\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.260\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.264\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.984466789095105\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.266\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.003001973626661325\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.267\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, True, False, False, True, 0.7453998923301697, 0.9595336318016052, True], reward: -0.003001973626661325\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.268\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.289\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.98446679 129.48695346  14.47997741  17.46693878   0.52002259\n",
      "   0.53306122   0.74612826   0.96080375]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.291\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.98446679 129.48695346  14.47997741  17.46693878   0.52002259\n",
      "   0.53306122   0.74612826   0.96080375]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.292\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.294\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.297\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9988, 129.4815,  14.4875,  17.4793,   0.5125,   0.5207,   0.2409,\n",
      "           0.6650]]), tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.7454, 0.9595,\n",
      "        1.0000]), tensor([[ 21.9845, 129.4870,  14.4800,  17.4669,   0.5200,   0.5331,   0.7461,\n",
      "           0.9608]]), tensor([-0.0030]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.301\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9988, 129.4815,  14.4875,  17.4793,   0.5125,   0.5207,   0.2409,\n",
      "           0.6650]]), action=tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.7454, 0.9595,\n",
      "        1.0000]), next_state=tensor([[ 21.9845, 129.4870,  14.4800,  17.4669,   0.5200,   0.5331,   0.7461,\n",
      "           0.9608]]), reward=tensor([-0.0030])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.302\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.304\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 14 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.314\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.337\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.97727789 129.49396624  14.47748188  17.45967994   0.52251812\n",
      "   0.54032006   0.74612826   0.96080375]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.338\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 15\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.339\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9526888958516185 at step 28\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.341\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3653, 0.5218,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.343\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, True, False, False, False, 0.3653028607368469, 0.5217710137367249, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.345\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, True, False, False, False, 0.3653028607368469, 0.5217710137367249, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.353\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.359\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.361\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.3653028607368469, yaw: 0.5217710137367249\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.364\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.368\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.978007226164067\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.370\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0006459562931038221\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.372\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, True, False, False, False, 0.3653028607368469, 0.5217710137367249, False], reward: -0.0006459562931038221\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.373\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.396\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.97800723 129.50698706  14.48179969  17.45706575   0.51820031\n",
      "   0.54293425   0.36651914   0.52098078]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.398\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.97800723 129.50698706  14.48179969  17.45706575   0.51820031\n",
      "   0.54293425   0.36651914   0.52098078]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.399\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.400\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.404\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9773, 129.4940,  14.4775,  17.4597,   0.5225,   0.5403,   0.7461,\n",
      "           0.9608]]), tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3653, 0.5218,\n",
      "        0.0000]), tensor([[ 21.9780, 129.5070,  14.4818,  17.4571,   0.5182,   0.5429,   0.3665,\n",
      "           0.5210]]), tensor([-0.0006]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.407\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9773, 129.4940,  14.4775,  17.4597,   0.5225,   0.5403,   0.7461,\n",
      "           0.9608]]), action=tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3653, 0.5218,\n",
      "        0.0000]), next_state=tensor([[ 21.9780, 129.5070,  14.4818,  17.4571,   0.5182,   0.5429,   0.3665,\n",
      "           0.5210]]), reward=tensor([-0.0006])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.409\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.410\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 15 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.429\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.456\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.99757069 129.5400165   14.50402708  17.46408977   0.49597292\n",
      "   0.53591023   0.36651914   0.52098078]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.458\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 16\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.459\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9509852223013566 at step 29\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.462\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m71\u001B[0m - \u001B[1mpolicy net output: tensor([1.3905e-03, 8.8223e-04, 7.4142e-01, 9.9853e-01, 5.2953e-01, 1.1488e-02,\n",
      "        8.7866e-01, 9.8278e-01, 6.7615e-01, 9.9885e-01])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.464\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mSelected action from policy network: tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9828, 0.6761,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.466\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, True, False, True, 0.9827849268913269, 0.6761465072631836, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.467\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, True, False, True, 0.9827849268913269, 0.6761465072631836, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.473\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.483\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.485\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9827849268913269, yaw: 0.6761465072631836\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.488\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.489\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.493\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.500\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.032452636077874\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.502\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.00544454099138072\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.504\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, True, False, True, 0.9827849268913269, 0.6761465072631836, True], reward: 0.00544454099138072\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.505\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.548\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.06421283 129.64501152  14.57761002  17.48971406   0.42238998\n",
      "   0.51028594   0.9817477    0.67544242]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.550\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.06421283 129.64501152  14.57761002  17.48971406   0.42238998\n",
      "   0.51028594   0.9817477    0.67544242]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.552\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.553\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.556\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9976, 129.5400,  14.5040,  17.4641,   0.4960,   0.5359,   0.3665,\n",
      "           0.5210]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9828, 0.6761,\n",
      "        1.0000]), tensor([[ 22.0642, 129.6450,  14.5776,  17.4897,   0.4224,   0.5103,   0.9817,\n",
      "           0.6754]]), tensor([0.0054]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.558\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9976, 129.5400,  14.5040,  17.4641,   0.4960,   0.5359,   0.3665,\n",
      "           0.5210]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9828, 0.6761,\n",
      "        1.0000]), next_state=tensor([[ 22.0642, 129.6450,  14.5776,  17.4897,   0.4224,   0.5103,   0.9817,\n",
      "           0.6754]]), reward=tensor([0.0054])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.560\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.561\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 16 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.570\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.591\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.09312914 129.69033627  14.60951266  17.50082374   0.39048734\n",
      "   0.49917626   0.9817477    0.67544242]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.593\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 17\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.594\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9492849526931186 at step 30\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.597\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.5847, 0.7274,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.599\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, True, True, False, 0.5846507549285889, 0.7273743152618408, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.601\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, True, True, False, 0.5846507549285889, 0.7273743152618408, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.610\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, True, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.621\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', True), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.622\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.5846507549285889, yaw: 0.7273743152618408\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.627\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.629\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.635\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.639\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.119454969139298\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.641\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.00870023330614238\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.643\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, True, True, False, 0.5846507549285889, 0.7273743152618408, True], reward: 0.00870023330614238\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.644\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.666\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.22852931 129.30541132  14.58074982  17.7          0.41925018\n",
      "   0.3          0.58381263   0.7278023 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.667\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.22852931 129.30541132  14.58074982  17.7          0.41925018\n",
      "   0.3          0.58381263   0.7278023 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.669\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.670\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.673\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0931, 129.6903,  14.6095,  17.5008,   0.3905,   0.4992,   0.9817,\n",
      "           0.6754]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.5847, 0.7274,\n",
      "        1.0000]), tensor([[ 22.2285, 129.3054,  14.5807,  17.7000,   0.4193,   0.3000,   0.5838,\n",
      "           0.7278]]), tensor([0.0087]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.676\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0931, 129.6903,  14.6095,  17.5008,   0.3905,   0.4992,   0.9817,\n",
      "           0.6754]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.5847, 0.7274,\n",
      "        1.0000]), next_state=tensor([[ 22.2285, 129.3054,  14.5807,  17.7000,   0.4193,   0.3000,   0.5838,\n",
      "           0.7278]]), reward=tensor([0.0087])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.677\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.678\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 17 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.684\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.708\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.22852931 129.30541132  14.58074982  17.7          0.41925018\n",
      "   0.3          0.58381263   0.7278023 ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.709\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 18\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.710\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9475880802258239 at step 31\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.713\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.7649, 0.7735,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.714\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, False, False, True, False, 0.7648606896400452, 0.7734623551368713, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.715\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, False, False, True, False, 0.7648606896400452, 0.7734623551368713, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.721\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, False, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.730\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.732\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7648606896400452, yaw: 0.7734623551368713\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.735\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.739\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.21184724944721\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.741\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.009239228030791224\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.742\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, False, False, True, False, 0.7648606896400452, 0.7734623551368713, False], reward: 0.009239228030791224\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.744\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.756\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.21184725 129.25281759  14.55439996  17.7          0.44560004\n",
      "   0.3          0.76445421   0.77230819]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.758\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.21184725 129.25281759  14.55439996  17.7          0.44560004\n",
      "   0.3          0.76445421   0.77230819]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.759\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.760\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.762\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.2285, 129.3054,  14.5807,  17.7000,   0.4193,   0.3000,   0.5838,\n",
      "           0.7278]]), tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.7649, 0.7735,\n",
      "        0.0000]), tensor([[ 22.2118, 129.2528,  14.5544,  17.7000,   0.4456,   0.3000,   0.7645,\n",
      "           0.7723]]), tensor([0.0092]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.764\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.2285, 129.3054,  14.5807,  17.7000,   0.4193,   0.3000,   0.5838,\n",
      "           0.7278]]), action=tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.7649, 0.7735,\n",
      "        0.0000]), next_state=tensor([[ 22.2118, 129.2528,  14.5544,  17.7000,   0.4456,   0.3000,   0.7645,\n",
      "           0.7723]]), reward=tensor([0.0092])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.766\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.767\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 18 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.774\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.795\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.19668286 129.20488863  14.53042159  17.7          0.46957841\n",
      "   0.3          0.76445421   0.77230819]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.796\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 19\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.797\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.94589459811198 at step 32\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.799\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7309, 0.2466,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.800\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, True, False, True, True, 0.7309246063232422, 0.24656324088573456, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.802\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, True, False, True, True, 0.7309246063232422, 0.24656324088573456, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.809\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, True, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.820\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.821\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7309246063232422, yaw: 0.24656324088573456\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.825\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.826\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.831\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.838\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.18289678137489\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.840\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.002895046807232049\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.840\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, True, False, True, True, 0.7309246063232422, 0.24656324088573456, True], reward: -0.002895046807232049\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.842\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.861\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.18289678 129.16121635  14.50860127  17.7          0.49139873\n",
      "   0.3          0.73042029   0.24609142]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.862\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.18289678 129.16121635  14.50860127  17.7          0.49139873\n",
      "   0.3          0.73042029   0.24609142]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.864\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.865\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.867\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.1967, 129.2049,  14.5304,  17.7000,   0.4696,   0.3000,   0.7645,\n",
      "           0.7723]]), tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7309, 0.2466,\n",
      "        1.0000]), tensor([[ 22.1829, 129.1612,  14.5086,  17.7000,   0.4914,   0.3000,   0.7304,\n",
      "           0.2461]]), tensor([-0.0029]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.870\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.1967, 129.2049,  14.5304,  17.7000,   0.4696,   0.3000,   0.7645,\n",
      "           0.7723]]), action=tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.7309, 0.2466,\n",
      "        1.0000]), next_state=tensor([[ 22.1829, 129.1612,  14.5086,  17.7000,   0.4914,   0.3000,   0.7304,\n",
      "           0.2461]]), reward=tensor([-0.0029])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.871\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.872\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 19 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.884\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.904\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.1715635  129.10191851  14.48364484  17.70569397   0.51635516\n",
      "   0.29430603   0.73042029   0.24609142]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.906\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 20\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.907\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9442044995776565 at step 33\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.909\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.2931, 0.1159,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.911\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, True, False, True, True, 0.2930779457092285, 0.1158844456076622, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.912\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, True, False, True, True, 0.2930779457092285, 0.1158844456076622, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.923\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, True, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.924\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.926\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.2930779457092285, yaw: 0.1158844456076622\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.928\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.929\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.933\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.942\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.162481391474223\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.944\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.002041538990066627\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.945\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, True, False, True, True, 0.2930779457092285, 0.1158844456076622, True], reward: -0.002041538990066627\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.947\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.967\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.21585501e+01 1.28945090e+02 1.44283178e+01 1.77337839e+01\n",
      " 5.71682192e-01 2.66216107e-01 2.93215314e-01 1.15191731e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.968\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.21585501e+01 1.28945090e+02 1.44283178e+01 1.77337839e+01\n",
      " 5.71682192e-01 2.66216107e-01 2.93215314e-01 1.15191731e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.970\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.971\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.973\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.1716, 129.1019,  14.4836,  17.7057,   0.5164,   0.2943,   0.7304,\n",
      "           0.2461]]), tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.2931, 0.1159,\n",
      "        1.0000]), tensor([[2.2159e+01, 1.2895e+02, 1.4428e+01, 1.7734e+01, 5.7168e-01, 2.6622e-01,\n",
      "         2.9322e-01, 1.1519e-01]]), tensor([-0.0020]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.976\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.1716, 129.1019,  14.4836,  17.7057,   0.5164,   0.2943,   0.7304,\n",
      "           0.2461]]), action=tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.2931, 0.1159,\n",
      "        1.0000]), next_state=tensor([[2.2159e+01, 1.2895e+02, 1.4428e+01, 1.7734e+01, 5.7168e-01, 2.6622e-01,\n",
      "         2.9322e-01, 1.1519e-01]]), reward=tensor([-0.0020])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.978\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.980\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 20 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.981\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 20\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:28.987\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.011\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.15855014 128.94509048  14.42831781  17.73378389   0.57168219\n",
      "   0.26621611   3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.013\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 21\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.015\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9425177778624569 at step 34\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.018\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2737, 0.7661,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.020\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, False, True, False, True, 0.2737007439136505, 0.7660608291625977, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.021\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, False, True, False, True, 0.2737007439136505, 0.7660608291625977, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.036\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, False, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.049\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.051\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.2737007439136505, yaw: 0.7660608291625977\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.053\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.055\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.057\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.064\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.01518762980611\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.066\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.014729376166811293\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.067\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, False, True, False, True, 0.2737007439136505, 0.7660608291625977, True], reward: -0.014729376166811293\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.068\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.095\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.01518763 129.50842797  14.50588     17.48539996   0.49412\n",
      "   0.51460004   0.27488936   0.76707221]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.097\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.01518763 129.50842797  14.50588     17.48539996   0.49412\n",
      "   0.51460004   0.27488936   0.76707221]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.098\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.100\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.102\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.1586, 128.9451,  14.4283,  17.7338,   0.5717,   0.2662,   3.1416,\n",
      "           0.0000]]), tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2737, 0.7661,\n",
      "        1.0000]), tensor([[ 22.0152, 129.5084,  14.5059,  17.4854,   0.4941,   0.5146,   0.2749,\n",
      "           0.7671]]), tensor([-0.0147]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.105\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.1586, 128.9451,  14.4283,  17.7338,   0.5717,   0.2662,   3.1416,\n",
      "           0.0000]]), action=tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.2737, 0.7661,\n",
      "        1.0000]), next_state=tensor([[ 22.0152, 129.5084,  14.5059,  17.4854,   0.4941,   0.5146,   0.2749,\n",
      "           0.7671]]), reward=tensor([-0.0147])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.107\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.108\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 21 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.116\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.146\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.00887672 129.52093562  14.50557156  17.47747389   0.49442844\n",
      "   0.52252611   0.27488936   0.76707221]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.147\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 22\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.148\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9408344262194922 at step 35\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.151\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.2896, 0.9740,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.153\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, True, False, True, 0.28955551981925964, 0.9740405678749084, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.155\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, True, False, True, 0.28955551981925964, 0.9740405678749084, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.162\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.175\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.177\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.28955551981925964, yaw: 0.9740405678749084\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.181\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.188\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.9791537000567\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.190\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0036033929749411443\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.191\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, True, False, True, 0.28955551981925964, 0.9740405678749084, False], reward: -0.0036033929749411443\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.193\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.217\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.97275239 129.5029946   14.47727537  17.4539854    0.52272463\n",
      "   0.5460146    0.29059732   0.97389372]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.218\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.97275239 129.5029946   14.47727537  17.4539854    0.52272463\n",
      "   0.5460146    0.29059732   0.97389372]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.220\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.221\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.223\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0089, 129.5209,  14.5056,  17.4775,   0.4944,   0.5225,   0.2749,\n",
      "           0.7671]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.2896, 0.9740,\n",
      "        0.0000]), tensor([[ 21.9728, 129.5030,  14.4773,  17.4540,   0.5227,   0.5460,   0.2906,\n",
      "           0.9739]]), tensor([-0.0036]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.226\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0089, 129.5209,  14.5056,  17.4775,   0.4944,   0.5225,   0.2749,\n",
      "           0.7671]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.2896, 0.9740,\n",
      "        0.0000]), next_state=tensor([[ 21.9728, 129.5030,  14.4773,  17.4540,   0.5227,   0.5460,   0.2906,\n",
      "           0.9739]]), reward=tensor([-0.0036])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.227\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.229\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 22 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.236\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.260\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.96292777 129.47189603  14.46182565  17.4539854    0.53817435\n",
      "   0.5460146    0.29059732   0.97389372]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.262\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 23\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.263\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9391544379153535 at step 36\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.266\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.1846, 0.2661,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.267\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, False, True, False, True, 0.18456800282001495, 0.2661072313785553, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.268\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, False, True, False, True, 0.18456800282001495, 0.2661072313785553, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.275\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, False, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.282\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', False), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.283\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.18456800282001495, yaw: 0.2661072313785553\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.286\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.292\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.9481639330506\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.294\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.003098976700609768\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.295\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, False, True, False, True, 0.18456800282001495, 0.2661072313785553, False], reward: -0.003098976700609768\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.297\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.322\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.93473029 129.44169163  14.43497248  17.43956714   0.56502752\n",
      "   0.56043286   0.18325957   0.26703538]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.324\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.93473029 129.44169163  14.43497248  17.43956714   0.56502752\n",
      "   0.56043286   0.18325957   0.26703538]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.326\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.328\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.331\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9629, 129.4719,  14.4618,  17.4540,   0.5382,   0.5460,   0.2906,\n",
      "           0.9739]]), tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.1846, 0.2661,\n",
      "        0.0000]), tensor([[ 21.9347, 129.4417,  14.4350,  17.4396,   0.5650,   0.5604,   0.1833,\n",
      "           0.2670]]), tensor([-0.0031]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.335\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9629, 129.4719,  14.4618,  17.4540,   0.5382,   0.5460,   0.2906,\n",
      "           0.9739]]), action=tensor([0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.1846, 0.2661,\n",
      "        0.0000]), next_state=tensor([[ 21.9347, 129.4417,  14.4350,  17.4396,   0.5650,   0.5604,   0.1833,\n",
      "           0.2670]]), reward=tensor([-0.0031])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.337\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.339\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 23 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.347\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.366\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.92990304 129.45206744  14.43497248  17.43331595   0.56502752\n",
      "   0.56668405   0.18325957   0.26703538]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.368\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 24\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.368\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9374778062300851 at step 37\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.371\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1334, 0.3482,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.373\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, False, False, 0.13340245187282562, 0.3481833338737488, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.374\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, False, False, 0.13340245187282562, 0.3481833338737488, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.382\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.390\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.391\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.13340245187282562, yaw: 0.3481833338737488\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.394\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.395\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.399\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.405\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.891397599722847\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.407\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.005676633332775439\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.409\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, False, False, 0.13340245187282562, 0.3481833338737488, True], reward: -0.005676633332775439\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.410\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.431\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.8913976  129.41850818  14.40060173  17.41172849   0.59939827\n",
      "   0.58827151   0.13351769   0.34819319]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.433\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.8913976  129.41850818  14.40060173  17.41172849   0.59939827\n",
      "   0.58827151   0.13351769   0.34819319]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.434\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.436\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.441\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9299, 129.4521,  14.4350,  17.4333,   0.5650,   0.5667,   0.1833,\n",
      "           0.2670]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1334, 0.3482,\n",
      "        1.0000]), tensor([[ 21.8914, 129.4185,  14.4006,  17.4117,   0.5994,   0.5883,   0.1335,\n",
      "           0.3482]]), tensor([-0.0057]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.445\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9299, 129.4521,  14.4350,  17.4333,   0.5650,   0.5667,   0.1833,\n",
      "           0.2670]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.1334, 0.3482,\n",
      "        1.0000]), next_state=tensor([[ 21.8914, 129.4185,  14.4006,  17.4117,   0.5994,   0.5883,   0.1335,\n",
      "           0.3482]]), reward=tensor([-0.0057])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.446\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.448\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 24 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.457\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.480\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.84310167 129.41121032  14.36778534  17.376185     0.63221466\n",
      "   0.623815     0.13351769   0.34819319]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.482\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 25\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.484\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9358045244571584 at step 38\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.486\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4337, 0.1835,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.488\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, False, False, False, 0.43366536498069763, 0.18349380791187286, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.489\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, False, False, False, 0.43366536498069763, 0.18349380791187286, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.499\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.506\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.508\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.43366536498069763, yaw: 0.18349380791187286\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.511\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.513\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.517\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.526\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.779165552950055\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.528\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.011223204677279242\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.529\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, False, False, False, 0.43366536498069763, 0.18349380791187286, True], reward: -0.011223204677279242\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.531\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.557\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.77916555 129.40642747  14.32578878  17.32794155   0.67421122\n",
      "   0.67205845   0.43458698   0.18325957]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.559\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.77916555 129.40642747  14.32578878  17.32794155   0.67421122\n",
      "   0.67205845   0.43458698   0.18325957]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.561\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.562\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.565\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.8431, 129.4112,  14.3678,  17.3762,   0.6322,   0.6238,   0.1335,\n",
      "           0.3482]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4337, 0.1835,\n",
      "        1.0000]), tensor([[ 21.7792, 129.4064,  14.3258,  17.3279,   0.6742,   0.6721,   0.4346,\n",
      "           0.1833]]), tensor([-0.0112]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.568\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.8431, 129.4112,  14.3678,  17.3762,   0.6322,   0.6238,   0.1335,\n",
      "           0.3482]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4337, 0.1835,\n",
      "        1.0000]), next_state=tensor([[ 21.7792, 129.4064,  14.3258,  17.3279,   0.6742,   0.6721,   0.4346,\n",
      "           0.1833]]), reward=tensor([-0.0112])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.569\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.571\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 25 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.585\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.608\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.70210904 129.41950421  14.28069872  17.26525812   0.71930128\n",
      "   0.73474188   0.43458698   0.18325957]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.610\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 26\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.612\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.934134585903444 at step 39\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.615\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.3803, 0.7457,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.617\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, False, False, True, True, 0.380327433347702, 0.7457314729690552, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.618\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, False, False, True, True, 0.380327433347702, 0.7457314729690552, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.638\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, False, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.651\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', False), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.652\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.380327433347702, yaw: 0.7457314729690552\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.656\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.657\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.660\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.667\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.514267669276524\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.668\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.02648978836735303\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.670\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, False, False, True, True, 0.380327433347702, 0.7457314729690552, True], reward: -0.02648978836735303\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.671\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.689\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.51426767 129.34032856  14.13844064  17.13900989   0.86155936\n",
      "   0.86099011   0.37960911   0.74612826]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.692\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.51426767 129.34032856  14.13844064  17.13900989   0.86155936\n",
      "   0.86099011   0.37960911   0.74612826]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.693\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.694\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.697\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7021, 129.4195,  14.2807,  17.2653,   0.7193,   0.7347,   0.4346,\n",
      "           0.1833]]), tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.3803, 0.7457,\n",
      "        1.0000]), tensor([[ 21.5143, 129.3403,  14.1384,  17.1390,   0.8616,   0.8610,   0.3796,\n",
      "           0.7461]]), tensor([-0.0265]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.699\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7021, 129.4195,  14.2807,  17.2653,   0.7193,   0.7347,   0.4346,\n",
      "           0.1833]]), action=tensor([1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.3803, 0.7457,\n",
      "        1.0000]), next_state=tensor([[ 21.5143, 129.3403,  14.1384,  17.1390,   0.8616,   0.8610,   0.3796,\n",
      "           0.7461]]), reward=tensor([-0.0265])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.700\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.701\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 26 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.712\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.730\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.44546539 129.1417583   14.03726212  17.13281456   0.96273788\n",
      "   0.86718544   0.37960911   0.74612826]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.731\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 27\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.733\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9324679838891854 at step 40\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.735\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0760, 0.3873,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.736\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, True, True, True, 0.07604329288005829, 0.3873428404331207, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.738\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, True, True, True, 0.07604329288005829, 0.3873428404331207, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.747\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, True, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.757\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.758\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.07604329288005829, yaw: 0.3873428404331207\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.762\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.767\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.374659253631943\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.769\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.013960841564458093\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.770\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, True, True, True, 0.07604329288005829, 0.3873428404331207, False], reward: -0.013960841564458093\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.772\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.787\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.13746593e+01 1.28982793e+02 1.39465195e+01 1.71152692e+01\n",
      " 1.05348046e+00 8.84730801e-01 7.59218225e-02 3.87463094e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.788\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.13746593e+01 1.28982793e+02 1.39465195e+01 1.71152692e+01\n",
      " 1.05348046e+00 8.84730801e-01 7.59218225e-02 3.87463094e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.789\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.790\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.793\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.4455, 129.1418,  14.0373,  17.1328,   0.9627,   0.8672,   0.3796,\n",
      "           0.7461]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0760, 0.3873,\n",
      "        0.0000]), tensor([[2.1375e+01, 1.2898e+02, 1.3947e+01, 1.7115e+01, 1.0535e+00, 8.8473e-01,\n",
      "         7.5922e-02, 3.8746e-01]]), tensor([-0.0140]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.795\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.4455, 129.1418,  14.0373,  17.1328,   0.9627,   0.8672,   0.3796,\n",
      "           0.7461]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0760, 0.3873,\n",
      "        0.0000]), next_state=tensor([[2.1375e+01, 1.2898e+02, 1.3947e+01, 1.7115e+01, 1.0535e+00, 8.8473e-01,\n",
      "         7.5922e-02, 3.8746e-01]]), reward=tensor([-0.0140])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.797\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.798\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 27 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.808\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.824\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.14281766e+01 1.28524740e+02 1.38465933e+01 1.72641044e+01\n",
      " 1.15340667e+00 7.35895565e-01 7.59218225e-02 3.87463094e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.825\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 28\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.827\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9308047117479722 at step 41\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.828\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0699, 0.3378,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.830\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, False, False, True, False, 0.06993626803159714, 0.3377671539783478, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.831\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, False, False, True, False, 0.06993626803159714, 0.3377671539783478, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.836\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, False, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.843\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.844\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.06993626803159714, yaw: 0.3377671539783478\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.847\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.849\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.852\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.857\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.439141538854123\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.858\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.006448228522218003\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.859\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, False, False, True, False, 0.06993626803159714, 0.3377671539783478, True], reward: 0.006448228522218003\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.859\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.882\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.14391415e+01 1.28321467e+02 1.37938332e+01 1.73199521e+01\n",
      " 1.20616680e+00 6.80047898e-01 7.06858347e-02 3.37721210e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.884\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.14391415e+01 1.28321467e+02 1.37938332e+01 1.73199521e+01\n",
      " 1.20616680e+00 6.80047898e-01 7.06858347e-02 3.37721210e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.885\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.886\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.889\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1428e+01, 1.2852e+02, 1.3847e+01, 1.7264e+01, 1.1534e+00, 7.3590e-01,\n",
      "         7.5922e-02, 3.8746e-01]]), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0699, 0.3378,\n",
      "        1.0000]), tensor([[2.1439e+01, 1.2832e+02, 1.3794e+01, 1.7320e+01, 1.2062e+00, 6.8005e-01,\n",
      "         7.0686e-02, 3.3772e-01]]), tensor([0.0064]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.892\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1428e+01, 1.2852e+02, 1.3847e+01, 1.7264e+01, 1.1534e+00, 7.3590e-01,\n",
      "         7.5922e-02, 3.8746e-01]]), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0699, 0.3378,\n",
      "        1.0000]), next_state=tensor([[2.1439e+01, 1.2832e+02, 1.3794e+01, 1.7320e+01, 1.2062e+00, 6.8005e-01,\n",
      "         7.0686e-02, 3.3772e-01]]), reward=tensor([0.0064])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.894\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.896\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 28 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.907\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.930\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.14304811e+01 1.28182419e+02 1.37476211e+01 1.73453571e+01\n",
      " 1.25237894e+00 6.54642892e-01 7.06858347e-02 3.37721210e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.931\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 29\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.933\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9291447628267139 at step 42\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.935\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0159, 0.9781,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.937\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, False, False, True, True, 0.015855371952056885, 0.9781131744384766, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.939\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, False, False, True, True, 0.015855371952056885, 0.9781131744384766, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.945\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, False, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.952\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.953\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.015855371952056885, yaw: 0.9781131744384766\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.956\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.960\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.42270985684078\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.962\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.001643168201334433\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.963\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, False, False, True, True, 0.015855371952056885, 0.9781131744384766, False], reward: -0.001643168201334433\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.964\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.983\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.14227099e+01 1.28055788e+02 1.37055680e+01 1.73684757e+01\n",
      " 1.29443199e+00 6.31524337e-01 1.57079633e-02 9.79129710e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.985\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.14227099e+01 1.28055788e+02 1.37055680e+01 1.73684757e+01\n",
      " 1.29443199e+00 6.31524337e-01 1.57079633e-02 9.79129710e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.986\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.987\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.990\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1430e+01, 1.2818e+02, 1.3748e+01, 1.7345e+01, 1.2524e+00, 6.5464e-01,\n",
      "         7.0686e-02, 3.3772e-01]]), tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0159, 0.9781,\n",
      "        0.0000]), tensor([[2.1423e+01, 1.2806e+02, 1.3706e+01, 1.7368e+01, 1.2944e+00, 6.3152e-01,\n",
      "         1.5708e-02, 9.7913e-01]]), tensor([-0.0016]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.992\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1430e+01, 1.2818e+02, 1.3748e+01, 1.7345e+01, 1.2524e+00, 6.5464e-01,\n",
      "         7.0686e-02, 3.3772e-01]]), action=tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0159, 0.9781,\n",
      "        0.0000]), next_state=tensor([[2.1423e+01, 1.2806e+02, 1.3706e+01, 1.7368e+01, 1.2944e+00, 6.3152e-01,\n",
      "         1.5708e-02, 9.7913e-01]]), reward=tensor([-0.0016])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.994\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:29.995\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 29 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.008\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.032\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.14157290e+01 1.27940475e+02 1.36672997e+01 1.73895135e+01\n",
      " 1.33270027e+00 6.10486451e-01 1.57079633e-02 9.79129710e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.034\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 30\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.035\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9274881304856122 at step 43\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.037\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.9695, 0.3086,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.039\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, False, False, True, 0.9694781303405762, 0.3086240291595459, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.040\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, False, False, True, 0.9694781303405762, 0.3086240291595459, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.049\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.063\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.064\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9694781303405762, yaw: 0.3086240291595459\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.068\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.073\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.4094519214149\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.075\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0013257935425880164\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.076\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, False, False, True, 0.9694781303405762, 0.3086240291595459, False], reward: -0.0013257935425880164\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.077\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.098\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.40945192 127.83547465  13.6324756   17.40865802   1.3675244\n",
      "   0.59134198   0.96865773   0.30892328]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.100\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.40945192 127.83547465  13.6324756   17.40865802   1.3675244\n",
      "   0.59134198   0.96865773   0.30892328]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.102\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.103\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.106\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1416e+01, 1.2794e+02, 1.3667e+01, 1.7390e+01, 1.3327e+00, 6.1049e-01,\n",
      "         1.5708e-02, 9.7913e-01]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.9695, 0.3086,\n",
      "        0.0000]), tensor([[ 21.4095, 127.8355,  13.6325,  17.4087,   1.3675,   0.5913,   0.9687,\n",
      "           0.3089]]), tensor([-0.0013]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.110\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1416e+01, 1.2794e+02, 1.3667e+01, 1.7390e+01, 1.3327e+00, 6.1049e-01,\n",
      "         1.5708e-02, 9.7913e-01]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.9695, 0.3086,\n",
      "        0.0000]), next_state=tensor([[ 21.4095, 127.8355,  13.6325,  17.4087,   1.3675,   0.5913,   0.9687,\n",
      "           0.3089]]), reward=tensor([-0.0013])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.111\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.113\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 30 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.115\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 30\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.128\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.160\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.163\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 31\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.164\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9258348080981359 at step 44\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.166\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.7966, 0.9049,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.168\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, True, True, False, True, 0.7965720295906067, 0.9049122929573059, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.171\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, True, True, False, True, 0.7965720295906067, 0.9049122929573059, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.179\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.186\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.188\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7965720295906067, yaw: 0.9049122929573059\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.194\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.196\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.200\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.207\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.02857381958474\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.208\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.061912189816984056\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.210\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, True, True, False, True, 0.7965720295906067, 0.9049122929573059, True], reward: 0.061912189816984056\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.211\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.238\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.02857382 129.47377352  14.50411402  17.5042011    0.49588598\n",
      "   0.4957989    0.79587014   0.90582588]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.240\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.02857382 129.47377352  14.50411402  17.5042011    0.49588598\n",
      "   0.4957989    0.79587014   0.90582588]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.242\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.243\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.246\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.7966, 0.9049,\n",
      "        1.0000]), tensor([[ 22.0286, 129.4738,  14.5041,  17.5042,   0.4959,   0.4958,   0.7959,\n",
      "           0.9058]]), tensor([0.0619]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.249\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   3.1416,\n",
      "           0.0000]]), action=tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.7966, 0.9049,\n",
      "        1.0000]), next_state=tensor([[ 22.0286, 129.4738,  14.5041,  17.5042,   0.4959,   0.4958,   0.7959,\n",
      "           0.9058]]), reward=tensor([0.0619])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.251\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.253\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 31 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.262\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.282\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.03976316 129.47628068  14.5119718   17.5122252    0.4880282\n",
      "   0.4877748    0.79587014   0.90582588]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.284\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 32\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.285\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9241847890509931 at step 45\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.287\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4413, 0.1005,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.288\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, False, True, True, False, 0.44128668308258057, 0.10048630833625793, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.289\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, False, True, True, False, 0.44128668308258057, 0.10048630833625793, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.294\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, False, True, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.313\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.315\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.44128668308258057, yaw: 0.10048630833625793\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.318\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.320\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.323\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.330\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.049945487167353\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.331\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.002137166758261344\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.333\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, False, True, True, False, 0.44128668308258057, 0.10048630833625793, True], reward: 0.002137166758261344\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.334\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.355\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20499455e+01 1.29478560e+02 1.45191224e+01 1.75195271e+01\n",
      " 4.80877622e-01 4.80472870e-01 4.42440965e-01 9.94837674e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.357\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.20499455e+01 1.29478560e+02 1.45191224e+01 1.75195271e+01\n",
      " 4.80877622e-01 4.80472870e-01 4.42440965e-01 9.94837674e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.358\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.359\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.361\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0398, 129.4763,  14.5120,  17.5122,   0.4880,   0.4878,   0.7959,\n",
      "           0.9058]]), tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4413, 0.1005,\n",
      "        1.0000]), tensor([[2.2050e+01, 1.2948e+02, 1.4519e+01, 1.7520e+01, 4.8088e-01, 4.8047e-01,\n",
      "         4.4244e-01, 9.9484e-02]]), tensor([0.0021]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.364\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0398, 129.4763,  14.5120,  17.5122,   0.4880,   0.4878,   0.7959,\n",
      "           0.9058]]), action=tensor([1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4413, 0.1005,\n",
      "        1.0000]), next_state=tensor([[2.2050e+01, 1.2948e+02, 1.4519e+01, 1.7520e+01, 4.8088e-01, 4.8047e-01,\n",
      "         4.4244e-01, 9.9484e-02]]), reward=tensor([0.0021])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.365\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.366\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 32 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.378\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.397\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20324471e+01 1.29451622e+02 1.45000001e+01 1.75126048e+01\n",
      " 4.99999918e-01 4.87395193e-01 4.42440965e-01 9.94837674e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.400\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 33\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.401\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9225380667441053 at step 46\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.404\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0795, 0.3151,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.406\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, False, False, True, True, 0.07948046177625656, 0.31508171558380127, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.407\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, False, False, True, True, 0.07948046177625656, 0.31508171558380127, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.415\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, False, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.423\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.424\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.07948046177625656, yaw: 0.31508171558380127\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.427\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.429\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.433\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.442\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.99347981167234\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.444\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0056465675495012135\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.446\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, False, False, True, True, 0.07948046177625656, 0.31508171558380127, True], reward: -0.0056465675495012135\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.447\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.467\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.19934798e+01 1.29398785e+02 1.44595723e+01 1.74953963e+01\n",
      " 5.40427719e-01 5.04603683e-01 7.85398163e-02 3.14159265e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.470\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.19934798e+01 1.29398785e+02 1.44595723e+01 1.74953963e+01\n",
      " 5.40427719e-01 5.04603683e-01 7.85398163e-02 3.14159265e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.472\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.473\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.476\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.2032e+01, 1.2945e+02, 1.4500e+01, 1.7513e+01, 5.0000e-01, 4.8740e-01,\n",
      "         4.4244e-01, 9.9484e-02]]), tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0795, 0.3151,\n",
      "        1.0000]), tensor([[2.1993e+01, 1.2940e+02, 1.4460e+01, 1.7495e+01, 5.4043e-01, 5.0460e-01,\n",
      "         7.8540e-02, 3.1416e-01]]), tensor([-0.0056]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.479\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.2032e+01, 1.2945e+02, 1.4500e+01, 1.7513e+01, 5.0000e-01, 4.8740e-01,\n",
      "         4.4244e-01, 9.9484e-02]]), action=tensor([1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0795, 0.3151,\n",
      "        1.0000]), next_state=tensor([[2.1993e+01, 1.2940e+02, 1.4460e+01, 1.7495e+01, 5.4043e-01, 5.0460e-01,\n",
      "         7.8540e-02, 3.1416e-01]]), reward=tensor([-0.0056])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.479\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.482\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 33 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.491\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.521\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.19580359e+01 1.29350540e+02 1.44227830e+01 1.74797366e+01\n",
      " 5.77217018e-01 5.20263410e-01 7.85398163e-02 3.14159265e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.522\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 34\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.523\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9208946345905813 at step 47\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.525\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6192, 0.3123,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.526\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, False, False, False, 0.6192297339439392, 0.3123219311237335, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.527\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, False, False, False, 0.6192297339439392, 0.3123219311237335, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.541\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.553\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.555\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6192297339439392, yaw: 0.3123219311237335\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.558\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.560\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.564\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.571\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.876472690387416\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.572\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.011700712128492441\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.574\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, False, False, False, 0.6192297339439392, 0.3123219311237335, True], reward: -0.011700712128492441\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.576\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.602\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.87647269 129.26518617  14.34585054  17.4373103    0.65414946\n",
      "   0.5626897    0.62046455   0.31154127]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.605\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.87647269 129.26518617  14.34585054  17.4373103    0.65414946\n",
      "   0.5626897    0.62046455   0.31154127]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.607\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.609\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.612\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1958e+01, 1.2935e+02, 1.4423e+01, 1.7480e+01, 5.7722e-01, 5.2026e-01,\n",
      "         7.8540e-02, 3.1416e-01]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6192, 0.3123,\n",
      "        1.0000]), tensor([[ 21.8765, 129.2652,  14.3459,  17.4373,   0.6541,   0.5627,   0.6205,\n",
      "           0.3115]]), tensor([-0.0117]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.615\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1958e+01, 1.2935e+02, 1.4423e+01, 1.7480e+01, 5.7722e-01, 5.2026e-01,\n",
      "         7.8540e-02, 3.1416e-01]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6192, 0.3123,\n",
      "        1.0000]), next_state=tensor([[ 21.8765, 129.2652,  14.3459,  17.4373,   0.6541,   0.5627,   0.6205,\n",
      "           0.3115]]), reward=tensor([-0.0117])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.616\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.618\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 34 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.627\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.651\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.81424189 129.25351534  14.3030235   17.39194161   0.6969765\n",
      "   0.60805839   0.62046455   0.31154127]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.653\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 35\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.654\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9192544860166902 at step 48\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.656\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.3767, 0.4180,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.658\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, False, True, False, False, 0.3767198622226715, 0.4179850220680237, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.659\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, False, True, False, False, 0.3767198622226715, 0.4179850220680237, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.666\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, False, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.672\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.673\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.3767198622226715, yaw: 0.4179850220680237\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.678\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.689\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.740258366240194\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.691\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.01362143241472218\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.693\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, False, True, False, False, 0.3767198622226715, 0.4179850220680237, False], reward: -0.01362143241472218\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.694\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.718\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.74025837 129.26902634  14.26076716  17.33092752   0.73923284\n",
      "   0.66907248   0.37699112   0.41887902]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.720\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.74025837 129.26902634  14.26076716  17.33092752   0.73923284\n",
      "   0.66907248   0.37699112   0.41887902]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.721\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.722\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.724\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.8142, 129.2535,  14.3030,  17.3919,   0.6970,   0.6081,   0.6205,\n",
      "           0.3115]]), tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.3767, 0.4180,\n",
      "        0.0000]), tensor([[ 21.7403, 129.2690,  14.2608,  17.3309,   0.7392,   0.6691,   0.3770,\n",
      "           0.4189]]), tensor([-0.0136]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.727\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.8142, 129.2535,  14.3030,  17.3919,   0.6970,   0.6081,   0.6205,\n",
      "           0.3115]]), action=tensor([1.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.3767, 0.4180,\n",
      "        0.0000]), next_state=tensor([[ 21.7403, 129.2690,  14.2608,  17.3309,   0.7392,   0.6691,   0.3770,\n",
      "           0.4189]]), reward=tensor([-0.0136])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.728\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.729\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 35 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.740\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.762\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.66746974 129.23236605  14.20395879  17.28334765   0.79604121\n",
      "   0.71665235   0.37699112   0.41887902]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.764\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 36\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.766\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9176176144618355 at step 49\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.768\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.5053, 0.1270,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.769\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, True, False, True, False, 0.5052753686904907, 0.12699806690216064, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.770\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, True, False, True, False, 0.5052753686904907, 0.12699806690216064, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.794\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, True, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.813\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.814\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.5052753686904907, yaw: 0.12699806690216064\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.818\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.821\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.61611909677759\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.823\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.012413926946260291\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.824\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, True, False, True, False, 0.5052753686904907, 0.12699806690216064, False], reward: -0.012413926946260291\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.825\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.840\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.16161191e+01 1.29197467e+02 1.41612801e+01 1.72518964e+01\n",
      " 8.38719925e-01 7.48103644e-01 5.05272818e-01 1.28281700e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.842\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.16161191e+01 1.29197467e+02 1.41612801e+01 1.72518964e+01\n",
      " 8.38719925e-01 7.48103644e-01 5.05272818e-01 1.28281700e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.843\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.844\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.846\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.6675, 129.2324,  14.2040,  17.2833,   0.7960,   0.7167,   0.3770,\n",
      "           0.4189]]), tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.5053, 0.1270,\n",
      "        0.0000]), tensor([[2.1616e+01, 1.2920e+02, 1.4161e+01, 1.7252e+01, 8.3872e-01, 7.4810e-01,\n",
      "         5.0527e-01, 1.2828e-01]]), tensor([-0.0124]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.848\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.6675, 129.2324,  14.2040,  17.2833,   0.7960,   0.7167,   0.3770,\n",
      "           0.4189]]), action=tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.5053, 0.1270,\n",
      "        0.0000]), next_state=tensor([[2.1616e+01, 1.2920e+02, 1.4161e+01, 1.7252e+01, 8.3872e-01, 7.4810e-01,\n",
      "         5.0527e-01, 1.2828e-01]]), reward=tensor([-0.0124])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.850\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.851\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 36 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.858\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.876\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.16492237e+01 1.29253220e+02 1.41985213e+01 1.72642299e+01\n",
      " 8.01478737e-01 7.35770147e-01 5.05272818e-01 1.28281700e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.877\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 37\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.878\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9159840133785289 at step 50\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.879\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4787, 0.5184,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.881\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, False, False, False, True, 0.47871559858322144, 0.5184481739997864, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.882\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, False, False, False, True, 0.47871559858322144, 0.5184481739997864, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.888\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, False, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.899\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.901\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.47871559858322144, yaw: 0.5184481739997864\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.904\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.905\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.909\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.915\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.677046613562844\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.916\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.006092751678525233\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.917\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, False, False, False, True, 0.47871559858322144, 0.5184481739997864, True], reward: 0.006092751678525233\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.918\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.935\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.67704661 129.31808666  14.23512127  17.27023534   0.76487873\n",
      "   0.72976466   0.47909288   0.51836279]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.938\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.67704661 129.31808666  14.23512127  17.27023534   0.76487873\n",
      "   0.72976466   0.47909288   0.51836279]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.939\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.940\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.943\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1649e+01, 1.2925e+02, 1.4199e+01, 1.7264e+01, 8.0148e-01, 7.3577e-01,\n",
      "         5.0527e-01, 1.2828e-01]]), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4787, 0.5184,\n",
      "        1.0000]), tensor([[ 21.6770, 129.3181,  14.2351,  17.2702,   0.7649,   0.7298,   0.4791,\n",
      "           0.5184]]), tensor([0.0061]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.946\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1649e+01, 1.2925e+02, 1.4199e+01, 1.7264e+01, 8.0148e-01, 7.3577e-01,\n",
      "         5.0527e-01, 1.2828e-01]]), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4787, 0.5184,\n",
      "        1.0000]), next_state=tensor([[ 21.6770, 129.3181,  14.2351,  17.2702,   0.7649,   0.7298,   0.4791,\n",
      "           0.5184]]), reward=tensor([0.0061])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.947\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.949\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 37 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.957\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.978\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.70007638 129.39124372  14.27113781  17.27048235   0.72886219\n",
      "   0.72951765   0.47909288   0.51836279]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.980\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 38\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.981\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9143536762323635 at step 51\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.983\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8066, 0.2330,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.985\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, False, False, False, False, 0.8066462278366089, 0.23301546275615692, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.986\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, False, False, False, False, 0.8066462278366089, 0.23301546275615692, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.992\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, False, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.995\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.997\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.8066462278366089, yaw: 0.23301546275615692\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:30.999\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.004\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.700076379769346\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.006\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.002302976620650199\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.007\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, False, False, False, False, 0.8066462278366089, 0.23301546275615692, False], reward: 0.002302976620650199\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.009\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.025\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.71941551 129.50961491  14.31805936  17.25691634   0.68194064\n",
      "   0.74308366   0.80634211   0.23300146]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.026\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.71941551 129.50961491  14.31805936  17.25691634   0.68194064\n",
      "   0.74308366   0.80634211   0.23300146]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.028\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.029\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.031\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7001, 129.3913,  14.2711,  17.2705,   0.7289,   0.7295,   0.4791,\n",
      "           0.5184]]), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8066, 0.2330,\n",
      "        0.0000]), tensor([[ 21.7194, 129.5096,  14.3181,  17.2569,   0.6819,   0.7431,   0.8063,\n",
      "           0.2330]]), tensor([0.0023]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.033\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7001, 129.3913,  14.2711,  17.2705,   0.7289,   0.7295,   0.4791,\n",
      "           0.5184]]), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8066, 0.2330,\n",
      "        0.0000]), next_state=tensor([[ 21.7194, 129.5096,  14.3181,  17.2569,   0.6819,   0.7431,   0.8063,\n",
      "           0.2330]]), reward=tensor([0.0023])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.034\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.036\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 38 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.044\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.071\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.71941551 129.50961491  14.31805936  17.25691634   0.68194064\n",
      "   0.74308366   0.80634211   0.23300146]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.073\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 39\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.074\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9127265965019891 at step 52\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.077\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6062, 0.4447,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.078\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, True, True, False, False, False, 0.6062425374984741, 0.4446912705898285, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.079\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, True, True, False, False, False, 0.6062425374984741, 0.4446912705898285, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.084\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, True, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.097\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.098\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6062425374984741, yaw: 0.4446912705898285\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.103\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.105\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.108\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.114\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.735673709104827\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.116\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.003559732933548077\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.117\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, True, True, False, False, False, 0.6062425374984741, 0.4446912705898285, True], reward: 0.003559732933548077\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.119\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.138\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.75061405 129.81321721  14.42663353  17.2074262    0.57336647\n",
      "   0.7925738    0.60737458   0.44505896]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.140\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.75061405 129.81321721  14.42663353  17.2074262    0.57336647\n",
      "   0.7925738    0.60737458   0.44505896]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.142\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.143\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.146\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7194, 129.5096,  14.3181,  17.2569,   0.6819,   0.7431,   0.8063,\n",
      "           0.2330]]), tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6062, 0.4447,\n",
      "        1.0000]), tensor([[ 21.7506, 129.8132,  14.4266,  17.2074,   0.5734,   0.7926,   0.6074,\n",
      "           0.4451]]), tensor([0.0036]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.150\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7194, 129.5096,  14.3181,  17.2569,   0.6819,   0.7431,   0.8063,\n",
      "           0.2330]]), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.6062, 0.4447,\n",
      "        1.0000]), next_state=tensor([[ 21.7506, 129.8132,  14.4266,  17.2074,   0.5734,   0.7926,   0.6074,\n",
      "           0.4451]]), reward=tensor([0.0036])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.151\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.153\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 39 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.163\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.186\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.76432989 129.94457495  14.47370696  17.18596924   0.52629304\n",
      "   0.81403076   0.60737458   0.44505896]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.187\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 40\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.189\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9111027676790844 at step 53\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.191\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4163, 0.4557,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.192\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, False, False, False, True, 0.4162944257259369, 0.4556860029697418, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.193\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, False, False, False, True, 0.4162944257259369, 0.4556860029697418, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.203\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, False, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.216\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.217\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.4162944257259369, yaw: 0.4556860029697418\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.221\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.222\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.227\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.235\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.035180601937437\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.237\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.029950689283261056\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.238\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, False, False, False, True, 0.4162944257259369, 0.4556860029697418, True], reward: 0.029950689283261056\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.239\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.262\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.0351806  129.51180038  14.5196      17.5          0.4804\n",
      "   0.5          0.41626103   0.45553093]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.264\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.0351806  129.51180038  14.5196      17.5          0.4804\n",
      "   0.5          0.41626103   0.45553093]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.266\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.267\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.270\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7643, 129.9446,  14.4737,  17.1860,   0.5263,   0.8140,   0.6074,\n",
      "           0.4451]]), tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4163, 0.4557,\n",
      "        1.0000]), tensor([[ 22.0352, 129.5118,  14.5196,  17.5000,   0.4804,   0.5000,   0.4163,\n",
      "           0.4555]]), tensor([0.0300]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.274\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7643, 129.9446,  14.4737,  17.1860,   0.5263,   0.8140,   0.6074,\n",
      "           0.4451]]), action=tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.4163, 0.4557,\n",
      "        1.0000]), next_state=tensor([[ 22.0352, 129.5118,  14.5196,  17.5000,   0.4804,   0.5000,   0.4163,\n",
      "           0.4555]]), reward=tensor([0.0300])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.276\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.277\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 40 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.279\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 40\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.295\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.323\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.04127546 129.54071704  14.53205811  17.49762246   0.46794189\n",
      "   0.50237754   0.41626103   0.45553093]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.325\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 41\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.327\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9094821832683316 at step 54\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.329\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7984, 0.5502,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.332\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, True, True, True, 0.7984429001808167, 0.5501598715782166, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.333\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, True, True, True, 0.7984429001808167, 0.5501598715782166, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.341\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, True, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.355\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', True), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.356\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7984429001808167, yaw: 0.5501598715782166\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.360\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.364\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.043236467499895\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.365\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0008055865562457854\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.367\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, True, True, True, 0.7984429001808167, 0.5501598715782166, False], reward: 0.0008055865562457854\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.368\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.392\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.10547151 128.96014875  14.39947188  17.68884964   0.60052812\n",
      "   0.31115036   0.79848813   0.54977871]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.394\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.10547151 128.96014875  14.39947188  17.68884964   0.60052812\n",
      "   0.31115036   0.79848813   0.54977871]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.395\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.397\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.399\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0413, 129.5407,  14.5321,  17.4976,   0.4679,   0.5024,   0.4163,\n",
      "           0.4555]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7984, 0.5502,\n",
      "        0.0000]), tensor([[ 22.1055, 128.9601,  14.3995,  17.6889,   0.6005,   0.3112,   0.7985,\n",
      "           0.5498]]), tensor([0.0008]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.401\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0413, 129.5407,  14.5321,  17.4976,   0.4679,   0.5024,   0.4163,\n",
      "           0.4555]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7984, 0.5502,\n",
      "        0.0000]), next_state=tensor([[ 22.1055, 128.9601,  14.3995,  17.6889,   0.6005,   0.3112,   0.7985,\n",
      "           0.5498]]), reward=tensor([0.0008])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.403\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.404\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 41 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.411\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.440\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.06658692 128.78893047  14.32368469  17.7          0.67631531\n",
      "   0.3          0.79848813   0.54977871]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.441\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 42\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.443\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9078648367873914 at step 55\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.445\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4453, 0.0923,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.446\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, False, True, True, False, 0.445313423871994, 0.09230582416057587, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.447\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, False, True, True, False, 0.445313423871994, 0.09230582416057587, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.457\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, False, True, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.465\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', False), ('jump', True), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.466\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.445313423871994, yaw: 0.09230582416057587\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.470\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.471\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.475\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.485\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.023448341912157\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.487\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.001978812558773768\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.488\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, False, True, True, False, 0.445313423871994, 0.09230582416057587, True], reward: -0.001978812558773768\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.489\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.507\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20234483e+01 1.28649079e+02 1.42547183e+01 1.77000000e+01\n",
      " 7.45281651e-01 3.00000000e-01 4.45058959e-01 9.16297857e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.511\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.20234483e+01 1.28649079e+02 1.42547183e+01 1.77000000e+01\n",
      " 7.45281651e-01 3.00000000e-01 4.45058959e-01 9.16297857e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.513\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.515\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.518\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0666, 128.7889,  14.3237,  17.7000,   0.6763,   0.3000,   0.7985,\n",
      "           0.5498]]), tensor([1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4453, 0.0923,\n",
      "        1.0000]), tensor([[2.2023e+01, 1.2865e+02, 1.4255e+01, 1.7700e+01, 7.4528e-01, 3.0000e-01,\n",
      "         4.4506e-01, 9.1630e-02]]), tensor([-0.0020]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.522\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0666, 128.7889,  14.3237,  17.7000,   0.6763,   0.3000,   0.7985,\n",
      "           0.5498]]), action=tensor([1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.4453, 0.0923,\n",
      "        1.0000]), next_state=tensor([[2.2023e+01, 1.2865e+02, 1.4255e+01, 1.7700e+01, 7.4528e-01, 3.0000e-01,\n",
      "         4.4506e-01, 9.1630e-02]]), reward=tensor([-0.0020])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.523\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.525\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 42 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.537\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.557\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.19843067e+01 1.28521337e+02 1.41919590e+01 1.77000000e+01\n",
      " 8.08041023e-01 3.00000000e-01 4.45058959e-01 9.16297857e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.558\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 43\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.560\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9062507217668754 at step 56\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.562\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3269, 0.5439,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.563\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, False, True, False, True, False, 0.32690685987472534, 0.5439099669456482, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.565\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, False, True, False, True, False, 0.32690685987472534, 0.5439099669456482, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.573\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, False, True, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.585\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.587\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.32690685987472534, yaw: 0.5439099669456482\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.591\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.596\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.948783077249878\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.598\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0074665264662279185\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.599\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, False, True, False, True, False, 0.32690685987472534, 0.5439099669456482, False], reward: -0.0074665264662279185\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.601\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.617\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.94878308 128.40469697  14.13484795  17.7          0.86515205\n",
      "   0.3          0.32724923   0.54454273]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.619\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.94878308 128.40469697  14.13484795  17.7          0.86515205\n",
      "   0.3          0.32724923   0.54454273]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.620\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.622\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.626\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1984e+01, 1.2852e+02, 1.4192e+01, 1.7700e+01, 8.0804e-01, 3.0000e-01,\n",
      "         4.4506e-01, 9.1630e-02]]), tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3269, 0.5439,\n",
      "        0.0000]), tensor([[ 21.9488, 128.4047,  14.1348,  17.7000,   0.8652,   0.3000,   0.3272,\n",
      "           0.5445]]), tensor([-0.0075]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.628\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1984e+01, 1.2852e+02, 1.4192e+01, 1.7700e+01, 8.0804e-01, 3.0000e-01,\n",
      "         4.4506e-01, 9.1630e-02]]), action=tensor([1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3269, 0.5439,\n",
      "        0.0000]), next_state=tensor([[ 21.9488, 128.4047,  14.1348,  17.7000,   0.8652,   0.3000,   0.3272,\n",
      "           0.5445]]), reward=tensor([-0.0075])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.630\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.631\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 43 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.641\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.659\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.94196383 128.28405472  14.09437642  17.72331869   0.90562358\n",
      "   0.27668131   0.32724923   0.54454273]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.661\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 44\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.662\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9046398317503215 at step 57\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.664\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.7392, 0.3184,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.666\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, False, True, False, False, False, 0.7391995787620544, 0.31840980052948, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.667\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, False, True, False, False, False, 0.7391995787620544, 0.31840980052948, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.675\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, False, True, False, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.684\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.686\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.7391995787620544, yaw: 0.31840980052948\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.689\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.690\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.693\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.699\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.9612825402253\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.701\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.0012499462975423372\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.702\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, False, True, False, False, False, 0.7391995787620544, 0.31840980052948, True], reward: 0.0012499462975423372\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.704\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.725\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.96128254 128.16019012  14.06904683  17.76785739   0.93095317\n",
      "   0.23214261   0.73827427   0.31939525]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.727\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.96128254 128.16019012  14.06904683  17.76785739   0.93095317\n",
      "   0.23214261   0.73827427   0.31939525]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.730\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.731\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.734\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9420, 128.2841,  14.0944,  17.7233,   0.9056,   0.2767,   0.3272,\n",
      "           0.5445]]), tensor([0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.7392, 0.3184,\n",
      "        1.0000]), tensor([[ 21.9613, 128.1602,  14.0690,  17.7679,   0.9310,   0.2321,   0.7383,\n",
      "           0.3194]]), tensor([0.0012]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.738\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9420, 128.2841,  14.0944,  17.7233,   0.9056,   0.2767,   0.3272,\n",
      "           0.5445]]), action=tensor([0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.7392, 0.3184,\n",
      "        1.0000]), next_state=tensor([[ 21.9613, 128.1602,  14.0690,  17.7679,   0.9310,   0.2321,   0.7383,\n",
      "           0.3194]]), reward=tensor([0.0012])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.739\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.740\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 44 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.749\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.769\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.99052839 128.0901665   14.06597471  17.80744548   0.93402529\n",
      "   0.19255452   0.73827427   0.31939525]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.771\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 45\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.772\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9030321602941674 at step 58\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.775\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2374, 0.4631,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.776\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, False, True, False, False, 0.23741574585437775, 0.46314749121665955, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.777\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, False, True, False, False, 0.23741574585437775, 0.46314749121665955, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.783\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, False, True, False, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.796\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', False), ('jump', True), ('sprint', False), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.798\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.23741574585437775, yaw: 0.46314749121665955\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.800\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.806\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.030465282620785\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.808\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.006918274239548339\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.809\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, False, True, False, False, 0.23741574585437775, 0.46314749121665955, False], reward: 0.006918274239548339\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.810\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.831\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.03739219 128.06066449  14.0859525   17.85132702   0.9140475\n",
      "   0.14867298   0.23823744   0.46338492]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.832\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 22.03739219 128.06066449  14.0859525   17.85132702   0.9140475\n",
      "   0.14867298   0.23823744   0.46338492]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.833\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.834\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.838\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9905, 128.0902,  14.0660,  17.8074,   0.9340,   0.1926,   0.7383,\n",
      "           0.3194]]), tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2374, 0.4631,\n",
      "        0.0000]), tensor([[ 22.0374, 128.0607,  14.0860,  17.8513,   0.9140,   0.1487,   0.2382,\n",
      "           0.4634]]), tensor([0.0069]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.841\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9905, 128.0902,  14.0660,  17.8074,   0.9340,   0.1926,   0.7383,\n",
      "           0.3194]]), action=tensor([0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.2374, 0.4631,\n",
      "        0.0000]), next_state=tensor([[ 22.0374, 128.0607,  14.0860,  17.8513,   0.9140,   0.1487,   0.2382,\n",
      "           0.4634]]), reward=tensor([0.0069])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.843\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.844\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 45 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.851\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.873\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.9899735  127.90259195  14.00890023  17.85132702   0.99109977\n",
      "   0.14867298   0.23823744   0.46338492]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.875\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 46\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.876\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.9014277009677254 at step 59\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.878\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6409, 0.2883,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.879\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, True, True, False, False, False, True, 0.6408994197845459, 0.28831690549850464, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.881\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, True, True, False, False, False, True, 0.6408994197845459, 0.28831690549850464, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.885\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, True, True, False, False, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.892\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.894\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.6408994197845459, yaw: 0.28831690549850464\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.908\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.911\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.917\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.928\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.911353219161523\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.931\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.011911206345926219\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.933\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, True, True, False, False, False, True, 0.6408994197845459, 0.28831690549850464, True], reward: -0.011911206345926219\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.935\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.955\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.91135322 127.67220681  13.89097363  17.84327608   1.10902637\n",
      "   0.15672392   0.6414085    0.28797933]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.957\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.91135322 127.67220681  13.89097363  17.84327608   1.10902637\n",
      "   0.15672392   0.6414085    0.28797933]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.958\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.959\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.962\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.9900, 127.9026,  14.0089,  17.8513,   0.9911,   0.1487,   0.2382,\n",
      "           0.4634]]), tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6409, 0.2883,\n",
      "        1.0000]), tensor([[ 21.9114, 127.6722,  13.8910,  17.8433,   1.1090,   0.1567,   0.6414,\n",
      "           0.2880]]), tensor([-0.0119]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.965\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.9900, 127.9026,  14.0089,  17.8513,   0.9911,   0.1487,   0.2382,\n",
      "           0.4634]]), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.6409, 0.2883,\n",
      "        1.0000]), next_state=tensor([[ 21.9114, 127.6722,  13.8910,  17.8433,   1.1090,   0.1567,   0.6414,\n",
      "           0.2880]]), reward=tensor([-0.0119])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.966\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.967\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 46 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.974\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.991\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.86661223 127.58768486  13.83808363  17.82755769   1.16191637\n",
      "   0.17244231   0.6414085    0.28797933]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.993\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 47\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.994\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.8998264473531556 at step 60\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.996\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0212, 0.2625,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.998\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, False, True, True, False, True, True, 0.02116769552230835, 0.2625320255756378, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:31.998\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, False, True, True, False, True, True, 0.02116769552230835, 0.2625320255756378, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.004\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, False, True, True, False, True, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.014\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', True), ('jump', False), ('sprint', True), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.015\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.02116769552230835, yaw: 0.2625320255756378\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.018\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.021\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.828014235186536\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.023\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.008333898397498629\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.024\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, False, True, True, False, True, True, 0.02116769552230835, 0.2625320255756378, False], reward: -0.008333898397498629\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.025\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.043\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.18280142e+01 1.27491157e+02 1.37853801e+01 1.78193787e+01\n",
      " 1.21461986e+00 1.80621257e-01 2.09439510e-02 2.61799388e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.045\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.18280142e+01 1.27491157e+02 1.37853801e+01 1.78193787e+01\n",
      " 1.21461986e+00 1.80621257e-01 2.09439510e-02 2.61799388e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.046\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.047\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.049\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.8666, 127.5877,  13.8381,  17.8276,   1.1619,   0.1724,   0.6414,\n",
      "           0.2880]]), tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0212, 0.2625,\n",
      "        0.0000]), tensor([[2.1828e+01, 1.2749e+02, 1.3785e+01, 1.7819e+01, 1.2146e+00, 1.8062e-01,\n",
      "         2.0944e-02, 2.6180e-01]]), tensor([-0.0083]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.052\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.8666, 127.5877,  13.8381,  17.8276,   1.1619,   0.1724,   0.6414,\n",
      "           0.2880]]), action=tensor([1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0212, 0.2625,\n",
      "        0.0000]), next_state=tensor([[2.1828e+01, 1.2749e+02, 1.3785e+01, 1.7819e+01, 1.2146e+00, 1.8062e-01,\n",
      "         2.0944e-02, 2.6180e-01]]), reward=tensor([-0.0083])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.053\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.054\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 47 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.060\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.081\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.18281725e+01 1.27490838e+02 1.37853801e+01 1.78195782e+01\n",
      " 1.21461986e+00 1.80421774e-01 2.09439510e-02 2.61799388e-01]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.083\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 48\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.084\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.8982283930454418 at step 61\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.086\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m71\u001B[0m - \u001B[1mpolicy net output: tensor([1.7072e-03, 9.5631e-04, 7.2994e-01, 9.9830e-01, 5.3891e-01, 1.1906e-02,\n",
      "        8.7455e-01, 9.8000e-01, 6.7223e-01, 9.9870e-01])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.089\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mSelected action from policy network: tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9800, 0.6722,\n",
      "        1.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.091\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, True, True, False, True, 0.9799971580505371, 0.6722313165664673, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.092\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, True, True, False, True, 0.9799971580505371, 0.6722313165664673, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.104\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.117\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.119\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.9799971580505371, yaw: 0.6722313165664673\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.123\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.124\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m52\u001B[0m - \u001B[34m\u001B[1mAttempting to swing\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.128\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mswing\u001B[0m:\u001B[36m58\u001B[0m - \u001B[1mSwing failed, no entity on cursor\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.135\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.772355223227283\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.137\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.0055659011959253496\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.139\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, True, True, False, True, 0.9799971580505371, 0.6722313165664673, True], reward: -0.0055659011959253496\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.141\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.172\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.77235522 127.29892723  13.69347043  17.81957823   1.30652957\n",
      "   0.18042177   0.97912971   0.67282443]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.175\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.77235522 127.29892723  13.69347043  17.81957823   1.30652957\n",
      "   0.18042177   0.97912971   0.67282443]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.177\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.179\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.182\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1828e+01, 1.2749e+02, 1.3785e+01, 1.7820e+01, 1.2146e+00, 1.8042e-01,\n",
      "         2.0944e-02, 2.6180e-01]]), tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9800, 0.6722,\n",
      "        1.0000]), tensor([[ 21.7724, 127.2989,  13.6935,  17.8196,   1.3065,   0.1804,   0.9791,\n",
      "           0.6728]]), tensor([-0.0056]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.186\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1828e+01, 1.2749e+02, 1.3785e+01, 1.7820e+01, 1.2146e+00, 1.8042e-01,\n",
      "         2.0944e-02, 2.6180e-01]]), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.9800, 0.6722,\n",
      "        1.0000]), next_state=tensor([[ 21.7724, 127.2989,  13.6935,  17.8196,   1.3065,   0.1804,   0.9791,\n",
      "           0.6728]]), reward=tensor([-0.0056])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.189\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.190\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 48 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.201\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.224\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.74823126 127.21541664  13.65362204  17.81957823   1.34637796\n",
      "   0.18042177   0.97912971   0.67282443]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.226\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 49\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.227\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.8966335316523644 at step 62\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.230\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3925, 0.5746,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.232\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, False, True, False, True, False, 0.39254650473594666, 0.5745538473129272, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.234\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, False, True, False, True, False, 0.39254650473594666, 0.5745538473129272, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.248\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, False, True, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.265\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', True), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.266\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.39254650473594666, yaw: 0.5745538473129272\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.272\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.280\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.73185272225841\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.282\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.004050250096887353\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.283\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, False, True, False, True, False, 0.39254650473594666, 0.5745538473129272, False], reward: -0.004050250096887353\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.285\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.304\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 21.73185272 127.06611119  13.5985729   17.84072694   1.4014271\n",
      "   0.15927306   0.39269908   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.307\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [ 21.73185272 127.06611119  13.5985729   17.84072694   1.4014271\n",
      "   0.15927306   0.39269908   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.309\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.311\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.314\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 21.7482, 127.2154,  13.6536,  17.8196,   1.3464,   0.1804,   0.9791,\n",
      "           0.6728]]), tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3925, 0.5746,\n",
      "        0.0000]), tensor([[ 21.7319, 127.0661,  13.5986,  17.8407,   1.4014,   0.1593,   0.3927,\n",
      "           0.0000]]), tensor([-0.0041]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.317\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 21.7482, 127.2154,  13.6536,  17.8196,   1.3464,   0.1804,   0.9791,\n",
      "           0.6728]]), action=tensor([0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.3925, 0.5746,\n",
      "        0.0000]), next_state=tensor([[ 21.7319, 127.0661,  13.5986,  17.8407,   1.4014,   0.1593,   0.3927,\n",
      "           0.0000]]), reward=tensor([-0.0041])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.318\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.319\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 49 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.327\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.349\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [ 22.00652651 129.42125057  14.47452     17.5          0.52548\n",
      "   0.5          3.14159265   0.        ]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.350\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 50\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.351\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.8950418567944758 at step 63\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.353\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.4204, 0.0219,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.355\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [False, False, True, False, False, True, False, 0.4203855097293854, 0.021906498819589615, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.356\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [False, False, True, False, False, True, False, 0.4203855097293854, 0.021906498819589615, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.364\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [False, False, True, False, False, True, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.368\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', False), ('jump', False), ('sprint', True), ('sneak', False)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.370\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.4203855097293854, yaw: 0.021906498819589615\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.373\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.379\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 22.007982806874743\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.380\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: 0.027613008461633372\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.381\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [False, False, True, False, False, True, False, 0.4203855097293854, 0.021906498819589615, False], reward: 0.027613008461633372\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.383\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.405\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.20079828e+01 1.29425862e+02 1.44768132e+01 1.75000000e+01\n",
      " 5.23186800e-01 5.00000000e-01 4.21497014e-01 2.09439510e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.407\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.20079828e+01 1.29425862e+02 1.44768132e+01 1.75000000e+01\n",
      " 5.23186800e-01 5.00000000e-01 4.21497014e-01 2.09439510e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.410\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.411\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.413\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[ 22.0065, 129.4212,  14.4745,  17.5000,   0.5255,   0.5000,   3.1416,\n",
      "           0.0000]]), tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.4204, 0.0219,\n",
      "        0.0000]), tensor([[2.2008e+01, 1.2943e+02, 1.4477e+01, 1.7500e+01, 5.2319e-01, 5.0000e-01,\n",
      "         4.2150e-01, 2.0944e-02]]), tensor([0.0276]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.417\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[ 22.0065, 129.4212,  14.4745,  17.5000,   0.5255,   0.5000,   3.1416,\n",
      "           0.0000]]), action=tensor([0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.4204, 0.0219,\n",
      "        0.0000]), next_state=tensor([[2.2008e+01, 1.2943e+02, 1.4477e+01, 1.7500e+01, 5.2319e-01, 5.0000e-01,\n",
      "         4.2150e-01, 2.0944e-02]]), reward=tensor([0.0276])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.419\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m91\u001B[0m - \u001B[1mNot enough memory to sample a batch\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.420\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m150\u001B[0m - \u001B[1mEpisode 50 finished after 1 steps\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.421\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m155\u001B[0m - \u001B[1mUpdated target network at episode 50\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.429\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.450\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.19851678e+01 1.29396312e+02 1.44535633e+01 1.74895754e+01\n",
      " 5.46436727e-01 5.10424553e-01 4.21497014e-01 2.09439510e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.452\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m130\u001B[0m - \u001B[1mStarting episode 51\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.453\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m66\u001B[0m - \u001B[34m\u001B[1mSelecting action with epsilon 0.8934533621050744 at step 64\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.455\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mselect_action\u001B[0m:\u001B[36m86\u001B[0m - \u001B[34m\u001B[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.5818, 0.0702,\n",
      "        0.0000])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.457\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m70\u001B[0m - \u001B[1mAction: [True, True, False, True, True, False, True, 0.5817515254020691, 0.07015133649110794, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.458\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m83\u001B[0m - \u001B[34m\u001B[1mUpdating agent state: [True, True, False, True, True, False, True, 0.5817515254020691, 0.07015133649110794, False]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.464\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m43\u001B[0m - \u001B[34m\u001B[1mSetting control state: [True, True, False, True, True, False, True]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.485\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mset_control_state\u001B[0m:\u001B[36m48\u001B[0m - \u001B[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', True), ('jump', True), ('sprint', False), ('sneak', True)])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.487\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m63\u001B[0m - \u001B[34m\u001B[1mLooking around with pitch: 0.5817515254020691, yaw: 0.07015133649110794\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.491\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mlook_around\u001B[0m:\u001B[36m65\u001B[0m - \u001B[1mLook around executed\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.495\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m109\u001B[0m - \u001B[34m\u001B[1mDistance to enemy updated: 21.850451131429207\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.496\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mupdate_agent_state\u001B[0m:\u001B[36m112\u001B[0m - \u001B[1mAgent state updated, Q value: -0.015753167544553647\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.497\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m75\u001B[0m - \u001B[34m\u001B[1mUpdated agent state with action [True, True, False, True, True, False, True, 0.5817515254020691, 0.07015133649110794, False], reward: -0.015753167544553647\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.499\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m117\u001B[0m - \u001B[34m\u001B[1mGetting model input\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.517\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.agents.minecraft_agent\u001B[0m:\u001B[36mget_model_input\u001B[0m:\u001B[36m125\u001B[0m - \u001B[1mModel input: [2.18504511e+01 1.29220409e+02 1.43161562e+01 1.74279663e+01\n",
      " 6.83843838e-01 5.72033685e-01 5.81194641e-01 7.06858347e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.518\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m78\u001B[0m - \u001B[34m\u001B[1mObtained observation: [2.18504511e+01 1.29220409e+02 1.43161562e+01 1.74279663e+01\n",
      " 6.83843838e-01 5.72033685e-01 5.81194641e-01 7.06858347e-02]\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.519\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m81\u001B[0m - \u001B[34m\u001B[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x00000130C0FF56A0>\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.520\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.environments.gym_1_vs_1\u001B[0m:\u001B[36mstep\u001B[0m:\u001B[36m88\u001B[0m - \u001B[1mStep result - Terminated: True, Truncated: False\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.523\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mpush\u001B[0m:\u001B[36m33\u001B[0m - \u001B[34m\u001B[1mPushed to memory: (tensor([[2.1985e+01, 1.2940e+02, 1.4454e+01, 1.7490e+01, 5.4644e-01, 5.1042e-01,\n",
      "         4.2150e-01, 2.0944e-02]]), tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.5818, 0.0702,\n",
      "        0.0000]), tensor([[2.1850e+01, 1.2922e+02, 1.4316e+01, 1.7428e+01, 6.8384e-01, 5.7203e-01,\n",
      "         5.8119e-01, 7.0686e-02]]), tensor([-0.0158]))\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.526\u001B[0m | \u001B[34m\u001B[1mDEBUG   \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36mtrain_dqn\u001B[0m:\u001B[36m143\u001B[0m - \u001B[34m\u001B[1mTransition added to memory: state=tensor([[2.1985e+01, 1.2940e+02, 1.4454e+01, 1.7490e+01, 5.4644e-01, 5.1042e-01,\n",
      "         4.2150e-01, 2.0944e-02]]), action=tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.5818, 0.0702,\n",
      "        0.0000]), next_state=tensor([[2.1850e+01, 1.2922e+02, 1.4316e+01, 1.7428e+01, 6.8384e-01, 5.7203e-01,\n",
      "         5.8119e-01, 7.0686e-02]]), reward=tensor([-0.0158])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.527\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m93\u001B[0m - \u001B[1mOptimizing model\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.530\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36msample\u001B[0m:\u001B[36m36\u001B[0m - \u001B[1mSampling 64 transitions from memory\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.533\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m102\u001B[0m - \u001B[1maction_batch shape: torch.Size([64, 10])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.535\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m103\u001B[0m - \u001B[1mstate_batch shape: torch.Size([64, 8])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.538\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m104\u001B[0m - \u001B[1mpolicy_net(state_batch) shape: torch.Size([64, 10])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.541\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m115\u001B[0m - \u001B[1mstate_action_values shape: torch.Size([64, 10])\u001B[0m\n",
      "\u001B[32m2024-06-01 20:56:32.543\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36msrc.train.train_dqn\u001B[0m:\u001B[36moptimize_model\u001B[0m:\u001B[36m116\u001B[0m - \u001B[1mexpected_state_action_values shape: torch.Size([64])\u001B[0m\n",
      "C:\\Users\\Przemek\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_dqn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmc_gym\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\bedwarsRL\\src\\train\\train_dqn.py:147\u001B[0m, in \u001B[0;36mtrain_dqn\u001B[1;34m(env, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, target_update, memory_capacity)\u001B[0m\n\u001B[0;32m    143\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTransition added to memory: state=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, action=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maction\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, next_state=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnext_state\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, reward=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreward\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    145\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m--> 147\u001B[0m \u001B[43moptimize_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[0;32m    150\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpisode \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepisode\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m finished after \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m steps\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\bedwarsRL\\src\\train\\train_dqn.py:117\u001B[0m, in \u001B[0;36mtrain_dqn.<locals>.optimize_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m    115\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_action_values shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate_action_values\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    116\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected_state_action_values shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_state_action_values\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 117\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMSELoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_action_values\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpected_state_action_values\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m    120\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\nn\\functional.py:3365\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3363\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[1;32m-> 3365\u001B[0m expanded_input, expanded_target \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3366\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mmse_loss(expanded_input, expanded_target, _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction))\n",
      "File \u001B[1;32m~\\miniforge3\\envs\\bedwarsRL\\lib\\site-packages\\torch\\functional.py:76\u001B[0m, in \u001B[0;36mbroadcast_tensors\u001B[1;34m(*tensors)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function(tensors):\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(broadcast_tensors, tensors, \u001B[38;5;241m*\u001B[39mtensors)\n\u001B[1;32m---> 76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbroadcast_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "e46ccd9a-9a93-45b6-a506-37272f61279a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T18:56:34.514132Z",
     "start_time": "2024-06-01T18:56:34.514132Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd1ed278-a324-49b6-864f-d6372f59418f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c00ebfaa-bcf1-430a-a0ac-5f323e659379",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b3f962683da89d6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#mc_gym.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd3c2a6ac8de199d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#mc_gym.reset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ad6ac7227209a3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
