{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:59:43.036264Z",
     "start_time": "2024-06-01T11:59:39.376087Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.gym_1_vs_1 import MinecraftGym\n",
    "from src.train.train_dqn import train_dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fa4289cc31a396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:59:53.629602Z",
     "start_time": "2024-06-01T11:59:43.038787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-01 15:06:20.657\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mInitializing MinecraftAgent with username bot1 and enemy bot2\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:20.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mMinecraftAgent initialized\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:25.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m22\u001b[0m - \u001b[34m\u001b[1mInitializing MinecraftAgent with username bot2 and enemy bot1\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:26.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mMinecraftAgent initialized\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mc_gym = MinecraftGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7125349cc894a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:59:55.524471Z",
     "start_time": "2024-06-01T11:59:53.631569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-01 15:06:31.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mStarting DQN training\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:31.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mUsing device: cuda\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:31.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mInitialized policy and target networks\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:32.282\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mInitialized ReplayMemory with capacity 1000000\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:32.286\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:32.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [5.38412794e+00 9.39938365e+01 1.50750000e+01 5.87105285e+00\n",
      " 7.50000000e-02 1.21289472e+01 3.14159265e+00 0.00000000e+00]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:32.294\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m126\u001b[0m - \u001b[1mStarting episode 0\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:32.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 1.0 at step 1\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2367, 0.2164,\n",
      "        0.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [True, False, True, True, True, False, False, 0.23670077323913574, 0.21637114882469177, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [True, False, True, True, True, False, False, 0.23670077323913574, 0.21637114882469177, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mInitial distance to enemy set: 22.02271554554524\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.631\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [True, False, True, True, True, False, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', True), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.636\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.23670077323913574, yaw: 0.21637114882469177\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.641\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.02271554554524\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [True, False, True, True, True, False, False, 0.23670077323913574, 0.21637114882469177, False], reward: 0.0\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.23561945   0.21729349]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [ 22.02271555 129.47245985  14.5         17.5          0.5\n",
      "   0.5          0.23561945   0.21729349]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9C484DE50>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.659\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[5.3841e+00, 9.3994e+01, 1.5075e+01, 5.8711e+00, 7.5000e-02, 1.2129e+01,\n",
      "         3.1416e+00, 0.0000e+00]], device='cuda:0'), tensor([1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2367, 0.2164,\n",
      "        0.0000], device='cuda:0'), tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.2356,\n",
      "           0.2173]], device='cuda:0'), tensor([0.], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[5.3841e+00, 9.3994e+01, 1.5075e+01, 5.8711e+00, 7.5000e-02, 1.2129e+01,\n",
      "         3.1416e+00, 0.0000e+00]], device='cuda:0'), action=tensor([1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2367, 0.2164,\n",
      "        0.0000], device='cuda:0'), next_state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.2356,\n",
      "           0.2173]], device='cuda:0'), reward=tensor([0.], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.669\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9982017988005998 at step 2\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.673\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3498, 0.5366,\n",
      "        1.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [True, True, True, True, False, False, False, 0.34975358843803406, 0.5365645289421082, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.676\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [True, True, True, True, False, False, False, 0.34975358843803406, 0.5365645289421082, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.680\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m92\u001b[0m - \u001b[1mInitial distance to enemy set: 22.02271554554524\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [True, True, True, True, False, False, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', True), ('right', True), ('jump', False), ('sprint', False), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.686\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.34975358843803406, yaw: 0.5365645289421082\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mAttempting to swing\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSwing failed, no entity on cursor\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.02271554554524\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.704\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [True, True, True, True, False, False, False, 0.34975358843803406, 0.5365645289421082, True], reward: 0.0\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.706\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.35081118   0.53668874]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.727\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [ 22.02271555 -50.52754015   0.5          0.5         14.5\n",
      "  17.5          0.35081118   0.53668874]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.728\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9CD3220D0>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.730\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.737\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.2356,\n",
      "           0.2173]], device='cuda:0'), tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3498, 0.5366,\n",
      "        1.0000], device='cuda:0'), tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.3508,\n",
      "           0.5367]], device='cuda:0'), tensor([0.], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.744\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[ 22.0227, 129.4725,  14.5000,  17.5000,   0.5000,   0.5000,   0.2356,\n",
      "           0.2173]], device='cuda:0'), action=tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.3498, 0.5366,\n",
      "        1.0000], device='cuda:0'), next_state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.3508,\n",
      "           0.5367]], device='cuda:0'), reward=tensor([0.], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.746\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.747\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9964071904095924 at step 3\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2529, 0.1276,\n",
      "        0.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, False, True, True, True, False, False, 0.2528746724128723, 0.12756088376045227, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.755\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, False, True, True, True, False, False, 0.2528746724128723, 0.12756088376045227, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, False, True, True, True, False, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', True), ('right', True), ('jump', True), ('sprint', False), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.2528746724128723, yaw: 0.12756088376045227\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.034524198613163\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.001180865306792356\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, False, True, True, True, False, False, 0.2528746724128723, 0.12756088376045227, False], reward: 0.001180865306792356\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [2.20345242e+01 1.29431772e+02 1.44954245e+01 1.75190585e+01\n",
      " 5.04575529e-01 4.80941550e-01 2.53945406e-01 1.28281700e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [2.20345242e+01 1.29431772e+02 1.44954245e+01 1.75190585e+01\n",
      " 5.04575529e-01 4.80941550e-01 2.53945406e-01 1.28281700e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.810\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9C484DE50>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.3508,\n",
      "           0.5367]], device='cuda:0'), tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2529, 0.1276,\n",
      "        0.0000], device='cuda:0'), tensor([[2.2035e+01, 1.2943e+02, 1.4495e+01, 1.7519e+01, 5.0458e-01, 4.8094e-01,\n",
      "         2.5395e-01, 1.2828e-01]], device='cuda:0'), tensor([0.0012], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.825\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[ 22.0227, -50.5275,   0.5000,   0.5000,  14.5000,  17.5000,   0.3508,\n",
      "           0.5367]], device='cuda:0'), action=tensor([0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.2529, 0.1276,\n",
      "        0.0000], device='cuda:0'), next_state=tensor([[2.2035e+01, 1.2943e+02, 1.4495e+01, 1.7519e+01, 5.0458e-01, 4.8094e-01,\n",
      "         2.5395e-01, 1.2828e-01]], device='cuda:0'), reward=tensor([0.0012], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.834\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9946161676485418 at step 4\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.841\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.6642, 0.4692,\n",
      "        0.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [True, True, False, True, False, False, True, 0.6642464995384216, 0.4692319929599762, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [True, True, False, True, False, False, True, 0.6642464995384216, 0.4692319929599762, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.878\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [True, True, False, True, False, False, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', True), ('back', True), ('left', False), ('right', True), ('jump', False), ('sprint', False), ('sneak', True)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.899\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.6642464995384216, yaw: 0.4692319929599762\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.055075221167968\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0032359675622728903\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.922\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [True, True, False, True, False, False, True, 0.6642464995384216, 0.4692319929599762, False], reward: 0.0032359675622728903\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.924\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [ 22.05825669 -50.67274339   0.50462718   0.50362817  14.49537282\n",
      "  17.49637183   0.66497045   0.4686209 ]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.937\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [ 22.05825669 -50.67274339   0.50462718   0.50362817  14.49537282\n",
      "  17.49637183   0.66497045   0.4686209 ]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.939\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9CD3220D0>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.941\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.948\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[2.2035e+01, 1.2943e+02, 1.4495e+01, 1.7519e+01, 5.0458e-01, 4.8094e-01,\n",
      "         2.5395e-01, 1.2828e-01]], device='cuda:0'), tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.6642, 0.4692,\n",
      "        0.0000], device='cuda:0'), tensor([[ 22.0583, -50.6727,   0.5046,   0.5036,  14.4954,  17.4964,   0.6650,\n",
      "           0.4686]], device='cuda:0'), tensor([0.0032], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.959\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[2.2035e+01, 1.2943e+02, 1.4495e+01, 1.7519e+01, 5.0458e-01, 4.8094e-01,\n",
      "         2.5395e-01, 1.2828e-01]], device='cuda:0'), action=tensor([1.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.6642, 0.4692,\n",
      "        0.0000], device='cuda:0'), next_state=tensor([[ 22.0583, -50.6727,   0.5046,   0.5036,  14.4954,  17.4964,   0.6650,\n",
      "           0.4686]], device='cuda:0'), reward=tensor([0.0032], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.962\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9928287233533546 at step 5\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.2903, 0.1012,\n",
      "        1.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.968\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, False, False, True, True, True, False, 0.2903132140636444, 0.1011558324098587, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, False, False, True, True, True, False, 0.2903132140636444, 0.1011558324098587, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.974\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, False, False, True, True, True, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', True), ('jump', True), ('sprint', True), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.2903132140636444, yaw: 0.1011558324098587\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.983\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mAttempting to swing\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.986\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSwing failed, no entity on cursor\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.053362221186596\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.991\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0018838022573433478\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.993\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, False, False, True, True, True, False, 0.2903132140636444, 0.1011558324098587, True], reward: 0.0018838022573433478\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:33.994\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [2.20533622e+01 1.29307994e+02 1.44840238e+01 1.75743874e+01\n",
      " 5.15976242e-01 4.25612566e-01 2.90597320e-01 1.02101761e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.004\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [2.20533622e+01 1.29307994e+02 1.44840238e+01 1.75743874e+01\n",
      " 5.15976242e-01 4.25612566e-01 2.90597320e-01 1.02101761e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9C484DE50>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.014\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[ 22.0583, -50.6727,   0.5046,   0.5036,  14.4954,  17.4964,   0.6650,\n",
      "           0.4686]], device='cuda:0'), tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.2903, 0.1012,\n",
      "        1.0000], device='cuda:0'), tensor([[2.2053e+01, 1.2931e+02, 1.4484e+01, 1.7574e+01, 5.1598e-01, 4.2561e-01,\n",
      "         2.9060e-01, 1.0210e-01]], device='cuda:0'), tensor([0.0019], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.022\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[ 22.0583, -50.6727,   0.5046,   0.5036,  14.4954,  17.4964,   0.6650,\n",
      "           0.4686]], device='cuda:0'), action=tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.2903, 0.1012,\n",
      "        1.0000], device='cuda:0'), next_state=tensor([[2.2053e+01, 1.2931e+02, 1.4484e+01, 1.7574e+01, 5.1598e-01, 4.2561e-01,\n",
      "         2.9060e-01, 1.0210e-01]], device='cuda:0'), reward=tensor([0.0019], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9910448503742513 at step 6\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.029\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9872, 0.9976,\n",
      "        1.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, False, False, False, False, False, True, 0.9872161149978638, 0.9976341724395752, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, False, False, False, False, False, True, 0.9872161149978638, 0.9976341724395752, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, False, False, False, False, False, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', False), ('left', False), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.041\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.9872161149978638, yaw: 0.9976341724395752\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.046\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mAttempting to swing\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.048\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSwing failed, no entity on cursor\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.052\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.086694352906715\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0031619131738747086\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.056\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, False, False, False, False, False, True, 0.9872161149978638, 0.9976341724395752, True], reward: 0.0031619131738747086\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.059\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [ 22.08669435 -50.62913518   0.54464348   0.53500496  14.45535652\n",
      "  17.46499504   0.98698369   0.99745567]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.067\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [ 22.08669435 -50.62913518   0.54464348   0.53500496  14.45535652\n",
      "  17.46499504   0.98698369   0.99745567]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.069\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9CD3220D0>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[2.2053e+01, 1.2931e+02, 1.4484e+01, 1.7574e+01, 5.1598e-01, 4.2561e-01,\n",
      "         2.9060e-01, 1.0210e-01]], device='cuda:0'), tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9872, 0.9976,\n",
      "        1.0000], device='cuda:0'), tensor([[ 22.0867, -50.6291,   0.5446,   0.5350,  14.4554,  17.4650,   0.9870,\n",
      "           0.9975]], device='cuda:0'), tensor([0.0032], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.086\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[2.2053e+01, 1.2931e+02, 1.4484e+01, 1.7574e+01, 5.1598e-01, 4.2561e-01,\n",
      "         2.9060e-01, 1.0210e-01]], device='cuda:0'), action=tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9872, 0.9976,\n",
      "        1.0000], device='cuda:0'), next_state=tensor([[ 22.0867, -50.6291,   0.5446,   0.5350,  14.4554,  17.4650,   0.9870,\n",
      "           0.9975]], device='cuda:0'), reward=tensor([0.0032], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9892645415757375 at step 7\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.1040, 0.3953,\n",
      "        1.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, True, True, False, True, True, False, 0.10401242226362228, 0.3953259587287903, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.096\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, True, True, False, True, True, False, 0.10401242226362228, 0.3953259587287903, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, True, True, False, True, True, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', True), ('sprint', True), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.105\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.10401242226362228, yaw: 0.3953259587287903\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.108\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mAttempting to swing\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mswing\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mSwing failed, no entity on cursor\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.128019696795743\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.007465747560914693\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.117\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, True, True, False, True, True, False, 0.10401242226362228, 0.3953259587287903, True], reward: 0.007465747560914693\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.118\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [2.21280197e+01 1.29449969e+02 1.46219035e+01 1.76351609e+01\n",
      " 3.78096534e-01 3.64839115e-01 1.04719755e-01 3.95317076e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.126\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [2.21280197e+01 1.29449969e+02 1.46219035e+01 1.76351609e+01\n",
      " 3.78096534e-01 3.64839115e-01 1.04719755e-01 3.95317076e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.127\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9C484DE50>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.149\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[ 22.0867, -50.6291,   0.5446,   0.5350,  14.4554,  17.4650,   0.9870,\n",
      "           0.9975]], device='cuda:0'), tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.1040, 0.3953,\n",
      "        1.0000], device='cuda:0'), tensor([[2.2128e+01, 1.2945e+02, 1.4622e+01, 1.7635e+01, 3.7810e-01, 3.6484e-01,\n",
      "         1.0472e-01, 3.9532e-01]], device='cuda:0'), tensor([0.0075], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.159\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[ 22.0867, -50.6291,   0.5446,   0.5350,  14.4554,  17.4650,   0.9870,\n",
      "           0.9975]], device='cuda:0'), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.1040, 0.3953,\n",
      "        1.0000], device='cuda:0'), next_state=tensor([[2.2128e+01, 1.2945e+02, 1.4622e+01, 1.7635e+01, 3.7810e-01, 3.6484e-01,\n",
      "         1.0472e-01, 3.9532e-01]], device='cuda:0'), reward=tensor([0.0075], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.162\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9874877898365757 at step 8\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.166\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9227, 0.3785,\n",
      "        0.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, True, True, False, False, False, True, 0.9226608872413635, 0.378500759601593, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.174\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, True, True, False, False, False, True, 0.9226608872413635, 0.378500759601593, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, True, True, False, False, False, True]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', True), ('right', False), ('jump', False), ('sprint', False), ('sneak', True)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.193\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.9226608872413635, yaw: 0.378500759601593\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.198\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.14735464335795\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.0060660290451235\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.201\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, True, True, False, False, False, True, 0.9226608872413635, 0.378500759601593, False], reward: 0.0060660290451235\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [ 22.21153796 -50.60349944   0.59317403   0.45691509  14.40682597\n",
      "  17.54308491   0.92153385   0.37960911]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.220\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [ 22.21153796 -50.60349944   0.59317403   0.45691509  14.40682597\n",
      "  17.54308491   0.92153385   0.37960911]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9CD3220D0>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.230\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[2.2128e+01, 1.2945e+02, 1.4622e+01, 1.7635e+01, 3.7810e-01, 3.6484e-01,\n",
      "         1.0472e-01, 3.9532e-01]], device='cuda:0'), tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9227, 0.3785,\n",
      "        0.0000], device='cuda:0'), tensor([[ 22.2115, -50.6035,   0.5932,   0.4569,  14.4068,  17.5431,   0.9215,\n",
      "           0.3796]], device='cuda:0'), tensor([0.0061], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.240\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[2.2128e+01, 1.2945e+02, 1.4622e+01, 1.7635e+01, 3.7810e-01, 3.6484e-01,\n",
      "         1.0472e-01, 3.9532e-01]], device='cuda:0'), action=tensor([0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9227, 0.3785,\n",
      "        0.0000], device='cuda:0'), next_state=tensor([[ 22.2115, -50.6035,   0.5932,   0.4569,  14.4068,  17.5431,   0.9215,\n",
      "           0.3796]], device='cuda:0'), reward=tensor([0.0061], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9857145880497566 at step 9\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m89\u001b[0m - \u001b[34m\u001b[1mSelected random action: tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0353, 0.9553,\n",
      "        0.0000], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mAction: [False, True, False, False, False, True, False, 0.035258371382951736, 0.9553123712539673, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m83\u001b[0m - \u001b[34m\u001b[1mUpdating agent state: [False, True, False, False, False, True, False, 0.035258371382951736, 0.9553123712539673, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m43\u001b[0m - \u001b[34m\u001b[1mSetting control state: [False, True, False, False, False, True, False]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mset_control_state\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mControl state set to: OrderedDict([('forward', False), ('back', True), ('left', False), ('right', False), ('jump', False), ('sprint', True), ('sneak', False)])\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.257\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1mLooking around with pitch: 0.035258371382951736, yaw: 0.9553123712539673\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mlook_around\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mLook around executed\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m109\u001b[0m - \u001b[34m\u001b[1mDistance to enemy updated: 22.211537958245888\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mupdate_agent_state\u001b[0m:\u001b[36m112\u001b[0m - \u001b[1mAgent state updated, Q value: 0.008351826145014484\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.263\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m75\u001b[0m - \u001b[34m\u001b[1mUpdated agent state with action [False, True, False, False, False, True, False, 0.035258371382951736, 0.9553123712539673, False], reward: 0.008351826145014484\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.264\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mGetting model input\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.agents.minecraft_agent\u001b[0m:\u001b[36mget_model_input\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mModel input: [2.22444970e+01 1.29363045e+02 1.47132642e+01 1.75803091e+01\n",
      " 2.86735816e-01 4.19690888e-01 3.40339204e-02 9.55567765e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.273\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m78\u001b[0m - \u001b[34m\u001b[1mObtained observation: [2.22444970e+01 1.29363045e+02 1.47132642e+01 1.75803091e+01\n",
      " 2.86735816e-01 4.19690888e-01 3.40339204e-02 9.55567765e-01]\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m81\u001b[0m - \u001b[34m\u001b[1mSwitched to next agent: <src.agents.minecraft_agent.MinecraftAgent object at 0x000001D9C484DE50>\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.environments.gym_1_vs_1\u001b[0m:\u001b[36mstep\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mStep result - Terminated: False, Truncated: False\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.282\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mpush\u001b[0m:\u001b[36m35\u001b[0m - \u001b[34m\u001b[1mPushed to memory: (tensor([[ 22.2115, -50.6035,   0.5932,   0.4569,  14.4068,  17.5431,   0.9215,\n",
      "           0.3796]], device='cuda:0'), tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0353, 0.9553,\n",
      "        0.0000], device='cuda:0'), tensor([[2.2244e+01, 1.2936e+02, 1.4713e+01, 1.7580e+01, 2.8674e-01, 4.1969e-01,\n",
      "         3.4034e-02, 9.5557e-01]], device='cuda:0'), tensor([0.0084], device='cuda:0'))\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.290\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mtrain_dqn\u001b[0m:\u001b[36m139\u001b[0m - \u001b[34m\u001b[1mTransition added to memory: state=tensor([[ 22.2115, -50.6035,   0.5932,   0.4569,  14.4068,  17.5431,   0.9215,\n",
      "           0.3796]], device='cuda:0'), action=tensor([0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0353, 0.9553,\n",
      "        0.0000], device='cuda:0'), next_state=tensor([[2.2244e+01, 1.2936e+02, 1.4713e+01, 1.7580e+01, 2.8674e-01, 4.1969e-01,\n",
      "         3.4034e-02, 9.5557e-01]], device='cuda:0'), reward=tensor([0.0084], device='cuda:0')\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36moptimize_model\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNot enough memory to sample a batch\u001b[0m\n",
      "\u001b[32m2024-06-01 15:06:34.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.train.train_dqn\u001b[0m:\u001b[36mselect_action\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mSelecting action with epsilon 0.9839449291224707 at step 10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_dqn(mc_gym, num_episodes=1000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ccd9a-9a93-45b6-a506-37272f61279a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:59:55.527463Z",
     "start_time": "2024-06-01T11:59:55.526466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ed278-a324-49b6-864f-d6372f59418f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ebfaa-bcf1-430a-a0ac-5f323e659379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f962683da89d6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#mc_gym.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c2a6ac8de199d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#mc_gym.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6ac7227209a3c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
